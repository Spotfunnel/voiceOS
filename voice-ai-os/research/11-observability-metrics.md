# Research: Observability & Metrics for Production Voice AI

**ðŸŸ¢ LOCKED** - Production-validated research based on Hamming.ai analysis (1M+ calls), five-layer observability stack, conversation quality metrics, real-time monitoring, production alerting patterns. Updated February 2026.

---

## Why This Matters for V1

Voice AI observability is fundamentally different from web application monitoring. Traditional APM tools miss what breaks in voice systems: conversation quality, not just response times. Analysis of 1M+ production voice agent calls across 50+ deployments (2024-2025) shows that voice agents need real-time monitoring to detect performance drift and quality degradation before customers abandon calls.

The critical difference: voice AI systems produce **partial outputs under time pressure**. An STT misrecognition ("fifteen" â†’ "fifty") cascades into a wrong database query, which produces an incorrect LLM response, which gets synthesized into confident-sounding but wrong audio. Without distributed tracing across the STT â†’ LLM â†’ TTS pipeline, you see a failed call but cannot determine which component caused the failure or at what timestamp.

Production teams report that 50% of voice AI incidents occur in telephony and audio layers (connection issues, codec problems, VAD failures), not in LLM intelligence. Jumping to LLM debugging first wastes critical incident response time. The 4-Stack Incident Response Framework (Telephony â†’ Audio â†’ Intelligence â†’ Output) reflects this reality, with target resolution times of SEV-1 <15 min, SEV-2 <30 min, SEV-3 <2 hours.

Standard web observability (request/response logs, HTTP status codes, error rates) is insufficient because voice conversations are **stateful, asynchronous, and interruptible**. A single "call" generates hundreds of events across multiple services with complex temporal relationships. Without correlation IDs propagated through all components, debugging becomes guesswork.

## What Matters in Production (Facts Only)

### Verified Observability Architecture (Shipped Systems)

**Five-Layer Observability Stack (Hamming AI, verified from 1M+ production calls):**

1. **Layer 1: Audio Pipeline**
   - Audio quality metrics (signal-to-noise ratio, clipping detection)
   - Frame drops and buffer underruns
   - Codec information and sample rate
   - Audio duration vs processing time

2. **Layer 2: STT Processing**
   - Transcription latency (time from audio end to transcript ready)
   - Word-level confidence scores
   - Word Error Rate (WER): 5-10% indicates good quality, 20%+ signals issues
   - Interim vs final transcript timing
   - Language detection confidence

3. **Layer 3: LLM Inference**
   - Token latency (time per token generated)
   - Prompt tokens and completion tokens
   - Model version and parameters (temperature, top_p)
   - Time to First Token (TTFT)
   - Tool/function call invocations and latency

4. **Layer 4: TTS Generation**
   - Synthesis latency (text to first audio byte)
   - Audio duration generated
   - Voice ID and model version
   - Character count processed
   - Word-level timestamps for synchronization

5. **Layer 5: End-to-End Trace**
   - Correlation IDs across all layers
   - Total latency breakdown by component
   - Turn-level timing (user speech end â†’ agent speech start)
   - Conversation-level context and state

**Critical Implementation Detail:** Generate a trace ID at audio capture and propagate it through all API calls for full correlation. This is the foundational requirement for distributed tracing.

### OpenTelemetry for Voice AI (Verified Implementation)

**Pipecat OpenTelemetry Support (documented, shipped):**
- Service-level spans for STT, TTS, LLM, and multimodal services (Gemini Live, OpenAI Realtime)
- Turn spans to correlate complete conversation turns
- Conversation spans to track entire conversations
- Usage metrics: LLM token counts, TTS character counts
- Multiple exporters: OTLP (Jaeger/Grafana), HTTP OTLP (Langfuse), console exporters
- Custom span attributes for enhanced context

**OpenTelemetry Semantic Conventions for GenAI (official specification):**
- Span names: `{gen_ai.operation.name} {gen_ai.request.model}` format
- Span kind: `CLIENT` for inference operations
- Required attributes: `gen_ai.operation.name`, `gen_ai.provider.name`
- Metrics: `gen_ai.client.token.usage` histogram with bucket boundaries from 1 to 16,777,216 tokens
- Context propagation via W3C TraceContext: `<version>-<trace-id>-<parent-id>-<trace-flags>`

**Trace Context Propagation (verified pattern):**
When one service calls another, it includes trace ID and span ID in headers (default: `traceparent` header). The receiving service creates a new span belonging to the same trace, setting the caller's span as its parent. This enables full request tracking across service boundaries, even across process and network boundaries.

**Instrumentation Overhead (measured):**
Expect 1-5% latency overhead from tracing instrumentation. This is acceptable for production voice systems where debugging capability outweighs minor latency increase.

### Production Observability Platforms (Verified Integrations)

**Pipecat-Supported Platforms:**
- **SigNoz**: Unified dashboards for correlated traces, logs, and metrics. Real-time visibility into latency, error rates, and usage trends.
- **Langfuse**: Open-source observability platform with native OpenTelemetry support for LLM applications.
- **Datadog**: Dedicated integration guide for metrics, logging, and traces. Pipecat Cloud provides specific Datadog configuration.
- **Jaeger/Grafana**: Via OTLP exporter for distributed tracing visualization.

**Roark (Voice AI-Specific Observability, verified 2025):**
- 40+ built-in call metrics: latency, repetitions, sentiment, instruction-following, tool call validation
- Multi-speaker analysis (up to 15 speakers)
- Automated evaluators: compliance, instruction-following, tool calls
- Threshold-based alerts and automated webhooks
- Call replay with transcript and audio timeline

**Hamming AI (Production Monitoring, verified 2025):**
- 50+ metrics tracked across 1,000+ calls per minute at scale
- Custom dashboards and scheduled reports
- Automated alerts for quality degradation
- Production call replay capabilities
- Custom evaluators for business-specific KPIs

**LiveKit Cloud (Built-in Observability, verified):**
- Unified timeline with transcripts, traces, logs, and actual audio recordings
- Turn-by-turn transcripts with tool calls and handoff events
- Metrics enrichment: token counts, latency per operation
- Session-level observability for debugging individual calls

**Vapi (Debugging Dashboard, verified):**
- Call logs, API logs, webhook logs
- Voice test suites and tool testing capabilities
- Call recording, logging, and transcribing with configurable storage

### Production Dashboards (Verified Metrics)

**VoiceAI Connect Enterprise (Grafana + InfluxDB, documented):**

*VoiceAI Default Dashboard:*
- Disk, memory, CPU usage per host
- Currently active calls and call metrics
- Failed calls per Session Manager
- Successful vs failed calls comparison

*VoiceAI Bots Dashboard:*
- Total failed and successful calls per bot
- Active calls per bot
- Bot messages and failures
- TTS characters and STT billing time per bot

*VoiceAI Providers Dashboard:*
- STT and TTS requests/failures per provider
- TTS delay metrics (max and average)

*Available KPIs:*
- Call attempts per minute
- Successful calls per minute
- Call endings per minute
- STT and TTS failures per minute
- Bot failures per minute

**Grafana AI Observability (verified 2025):**
- AI-specific metrics dashboards
- Integration with Datadog data sources
- Custom visualization for voice AI pipelines

### Critical Latency Targets (Production Benchmarks)

**Time-to-First-Byte (TTFB):**
- Target: P95 <300ms for conversational responses
- TTS TTFB: P95 <150ms (per D-TS-002)
- Requires dual-streaming, sentence-level chunking, warm connections

**End-to-End Turn Latency:**
- P50 <500ms, P95 <800ms (per D-LT-001)
- P99 <1000ms
- Human-normal response time: 300-1,200 milliseconds
- Conversation quality degrades measurably beyond these thresholds

**Component-Level Targets:**
- STT Real-Time Factor (RTF): <1.0 in production, alert at >0.8 (per D-LT-005)
- LLM Time to First Token: <200ms for conversational quality
- TTS Time to First Audio: ~90ms (Cartesia Sonic 3 baseline)

### Minimum Event Set for Call Replay (Verified Requirements)

**Essential Events (cannot replay without these):**

1. **Call Initialization:**
   - Call ID (correlation ID for entire conversation)
   - Timestamp (call start)
   - Caller information (phone number, region, device type)
   - Agent configuration (model versions, voice ID, system prompt hash)

2. **Audio Events:**
   - Audio chunk received (timestamp, duration, sample rate, codec)
   - Audio quality metrics (SNR, clipping, silence detection)
   - VAD events (speech start, speech end, confidence)

3. **STT Events:**
   - Interim transcripts (timestamp, text, confidence)
   - Final transcripts (timestamp, text, word-level confidence, word timestamps)
   - Language detection (detected language, confidence)
   - STT latency (audio end â†’ transcript ready)

4. **Turn Detection Events:**
   - End-of-utterance detected (timestamp, silence duration, VAD confidence)
   - End-of-turn detected (timestamp, detection method: VAD/STT/semantic)
   - Barge-in detected (timestamp, word count, confidence)

5. **LLM Events:**
   - Request sent (timestamp, prompt tokens, model, temperature)
   - First token received (timestamp, TTFT)
   - Streaming tokens (timestamps, token IDs, cumulative latency)
   - Request completed (timestamp, completion tokens, finish reason)
   - Tool/function calls (timestamp, function name, arguments, results)

6. **TTS Events:**
   - Synthesis request (timestamp, text, character count, voice ID)
   - First audio byte (timestamp, TTFB)
   - Audio chunks generated (timestamps, duration, byte size)
   - Word-level timestamps (word, start time, end time, character indices)

7. **Playback Events:**
   - Audio playback started (timestamp, chunk ID)
   - Audio playback interrupted (timestamp, delivered audio duration, reason)
   - Audio playback completed (timestamp, total duration)

8. **State Changes:**
   - Conversation state updates (timestamp, state name, context snapshot)
   - Interruption handling (timestamp, delivered audio, discarded audio, LLM context adjustment)

9. **Error Events:**
   - Component failures (timestamp, component, error type, error message, retry count)
   - Timeout events (timestamp, component, expected duration, actual duration)
   - Rate limit hits (timestamp, provider, limit type, retry-after)

10. **Call Termination:**
    - Call end (timestamp, duration, termination reason)
    - Final metrics (total turns, total tokens, total audio duration, cost)

**Storage Format:**
Events must be stored with nanosecond-precision timestamps and correlation IDs. JSON or Protocol Buffers are common formats. Total storage per call: 50-500 KB depending on call duration and verbosity.

### Metrics That Predict Bad User Experience (Verified Signals)

**Leading Indicators (detect before user complaints):**

1. **Latency Degradation:**
   - P95 turn latency increasing over time (trend analysis)
   - TTFT exceeding 200ms consistently
   - TTS TTFB exceeding 150ms at P95
   - **Why it matters:** Users perceive latency >300-400ms as awkward, >1.5s as broken

2. **STT Quality Degradation:**
   - Word-level confidence scores dropping below 0.7
   - Increasing interim-to-final transcript divergence
   - WER increasing above 10%
   - **Why it matters:** Low-confidence transcripts lead to incorrect LLM responses

3. **Interruption Handling Failures:**
   - Barge-in detection latency >100ms
   - Audio cancellation latency >100ms (per D-TS-005)
   - Increasing false-positive barge-ins (<2 words)
   - **Why it matters:** Poor interruption handling breaks conversational flow

4. **LLM Response Quality:**
   - Increasing token count per response (verbosity)
   - Repetition detection (same phrases across turns)
   - Tool call failure rate increasing
   - **Why it matters:** Verbose or repetitive responses frustrate users

5. **Audio Quality Issues:**
   - Buffer underruns increasing
   - Frame drops above 1% of packets
   - Jitter exceeding adaptive buffer capacity
   - **Why it matters:** Audio quality issues cause user drop-off

6. **Provider Reliability:**
   - Increasing 429 rate limit errors
   - Increasing timeout errors from specific providers
   - Fallback stack activation frequency increasing
   - **Why it matters:** Provider issues cascade to user experience

**Lagging Indicators (user already experienced bad quality):**
- Call abandonment rate (user hangs up mid-conversation)
- Average call duration decreasing over time
- Retry rate (user calls back immediately after failed call)
- Explicit user feedback (if collected)

### Signals Teams Think They Need But Don't (Verified Observations)

**Over-Collected Metrics:**

1. **Full Audio Recording for Every Call:**
   - **Why teams want it:** Complete replay capability
   - **Why it's excessive:** Storage costs scale linearly with call volume. 1-minute call = ~1 MB audio. 10,000 calls/day = 10 GB/day = 300 GB/month = $6-15/month storage + egress costs
   - **What's sufficient:** Record audio for failed calls and random 1-5% sample for quality monitoring

2. **Token-Level LLM Traces:**
   - **Why teams want it:** Detailed LLM behavior analysis
   - **Why it's excessive:** Token-level streaming generates massive event volume (100+ events per response). Adds significant observability overhead.
   - **What's sufficient:** First token, every 10th token, and final token with cumulative metrics

3. **Real-Time Sentiment Analysis:**
   - **Why teams want it:** Detect frustrated users during calls
   - **Why it's excessive:** Adds 50-100ms latency per turn for sentiment model inference. Rarely actionable in real-time.
   - **What's sufficient:** Post-call sentiment analysis on transcripts for quality monitoring

4. **Per-Word STT Confidence Scores:**
   - **Why teams want it:** Identify specific misrecognitions
   - **Why it's excessive:** Storage overhead for marginal debugging value
   - **What's sufficient:** Sentence-level average confidence with flag for low-confidence utterances

5. **Full LLM Prompt Logging:**
   - **Why teams want it:** Reproduce exact LLM inputs
   - **Why it's excessive:** Prompts can be 1,000-10,000+ tokens with conversation history. Massive storage cost.
   - **What's sufficient:** System prompt hash + conversation turn count + last 3 turns for context

### Data Too Expensive or Slow to Collect in Real Time (Verified Constraints)

**Prohibitively Expensive:**

1. **Full Audio Recording (all calls):**
   - Cost: ~$6-15/month per 10,000 calls for storage + egress
   - Alternative: Record 1-5% sample + all failed calls

2. **Complete LLM Prompt History:**
   - Cost: Scales with conversation length (quadratic token growth)
   - Alternative: Prompt hash + turn count + sliding window of last N turns

3. **Video Recording (for multimodal agents):**
   - Cost: 10-50x audio storage costs
   - Alternative: Keyframe extraction + metadata

**Too Slow for Real-Time Collection:**

1. **Detailed Semantic Analysis:**
   - Latency: 50-200ms per turn for intent classification, entity extraction, sentiment
   - Impact: Breaks conversational latency budget
   - Alternative: Async post-call analysis pipeline

2. **WER Calculation (Word Error Rate):**
   - Latency: Requires ground truth transcripts, not available in real-time
   - Alternative: Confidence scores as proxy, batch WER calculation on test sets

3. **Cross-Call Pattern Analysis:**
   - Latency: Requires aggregation across multiple calls
   - Alternative: Stream events to data warehouse, run batch analytics

4. **Root Cause Analysis:**
   - Latency: Requires correlation across multiple systems and historical data
   - Alternative: Collect structured events, run RCA post-incident

**Observability Overhead Limits:**

Tracing instrumentation adds 1-5% latency overhead. Beyond this threshold, observability degrades the system it's meant to monitor. Production teams report that excessive logging (debug-level logs in production) can add 10-20ms per operation, breaking latency budgets.

### Differences: Logs vs Traces vs Events vs Metrics

**Logs (unstructured or semi-structured text):**
- **What:** Human-readable messages about system state
- **When:** Debugging specific component behavior, error messages
- **Voice AI example:** "STT provider returned 429 rate limit error, retrying in 2s"
- **Limitation:** Hard to correlate across services without correlation IDs
- **Storage:** Text-based, high volume, expensive to search at scale

**Traces (distributed request flows):**
- **What:** Causal relationships between operations across services
- **When:** Understanding end-to-end latency, identifying bottlenecks
- **Voice AI example:** Audio capture â†’ STT â†’ LLM â†’ TTS â†’ Playback with parent-child span relationships
- **Limitation:** Requires instrumentation across all services
- **Storage:** Structured spans with timestamps and attributes, moderate volume

**Events (discrete state changes):**
- **What:** Timestamped facts about what happened
- **When:** Reconstructing call timelines, replay, audit trails
- **Voice AI example:** "Barge-in detected at 00:23.456, interrupted TTS playback, delivered 2.3s of 5.1s response"
- **Limitation:** High cardinality, requires careful schema design
- **Storage:** Structured JSON/Protobuf, high volume, requires time-series database

**Metrics (aggregated numerical data):**
- **What:** Quantitative measurements over time windows
- **When:** Dashboards, alerting, capacity planning, SLA monitoring
- **Voice AI example:** "P95 turn latency: 650ms over last 5 minutes"
- **Limitation:** Loses individual call details, cannot replay specific calls
- **Storage:** Time-series data, low volume, efficient for aggregation

**For Voice AI, the hierarchy is:**
1. **Events** are foundational (required for replay)
2. **Traces** provide structure (required for debugging)
3. **Metrics** enable monitoring (required for alerting)
4. **Logs** supplement debugging (useful but not sufficient)

### How Production Teams Correlate STT, LLM, TTS, Telephony Timelines

**Correlation ID Propagation (verified pattern):**

1. **Call-Level Correlation ID:**
   - Generated at telephony layer when call connects
   - Format: UUID or timestamp-based unique identifier
   - Propagated to all downstream services (STT, LLM, TTS)
   - Stored in every event, span, log, and metric

2. **Turn-Level Correlation ID:**
   - Generated when user speech ends (end-of-turn detection)
   - Links: user audio â†’ STT transcript â†’ LLM request â†’ TTS synthesis â†’ agent audio
   - Enables turn-by-turn replay and latency analysis

3. **Span Hierarchy (OpenTelemetry pattern):**
   ```
   Call Span (root)
   â”œâ”€â”€ Turn 1 Span
   â”‚   â”œâ”€â”€ STT Span (user speech)
   â”‚   â”œâ”€â”€ LLM Span (inference)
   â”‚   â””â”€â”€ TTS Span (agent speech)
   â”œâ”€â”€ Turn 2 Span
   â”‚   â”œâ”€â”€ STT Span
   â”‚   â”œâ”€â”€ Barge-in Span (interruption)
   â”‚   â””â”€â”€ TTS Cancellation Span
   â””â”€â”€ Turn 3 Span
       â”œâ”€â”€ STT Span
       â”œâ”€â”€ LLM Span
       â””â”€â”€ TTS Span
   ```

4. **Timestamp Synchronization:**
   - All services use synchronized clocks (NTP)
   - Timestamps stored with nanosecond precision
   - Timezone: UTC for all events
   - Clock skew detection and correction in post-processing

5. **Context Propagation Headers:**
   - W3C TraceContext `traceparent` header: `<version>-<trace-id>-<parent-id>-<trace-flags>`
   - Custom headers for voice-specific context: call-id, turn-id, user-id
   - Propagated through HTTP headers, WebSocket messages, message queue metadata

**Timeline Reconstruction (verified process):**

1. Query all events with call-id correlation ID
2. Sort events by timestamp (nanosecond precision)
3. Group events by turn-id to reconstruct turn boundaries
4. Link spans using parent-child relationships
5. Calculate latency deltas between consecutive events
6. Identify gaps or overlaps indicating concurrency or errors
7. Render timeline visualization with swimlanes per component

**Tools for Timeline Visualization:**
- Jaeger UI: Distributed tracing timeline with span waterfall
- Grafana Tempo: Trace visualization with logs correlation
- Langfuse: LLM-specific trace visualization with token counts
- LiveKit Cloud: Voice-specific timeline with audio playback

### Observability Gaps That Cause Blind Spots (Verified Issues)

**Gap 1: Missing Interruption Context**
- **Problem:** Logs show "TTS playback interrupted" but don't record how much audio was delivered
- **Impact:** Cannot determine if LLM context needs adjustment, cannot replay conversation accurately
- **Solution:** Record delivered audio duration and word-level timestamp at interruption point (per D-TT-006)

**Gap 2: Lack of Word-Level Timestamps**
- **Problem:** TTS generates audio but doesn't provide word-level timestamps
- **Impact:** Cannot synchronize conversation state after interruptions, cannot determine which words were heard
- **Solution:** Enable word-level timestamps on every TTS request (per D-TS-004)

**Gap 3: STT Interim vs Final Transcript Divergence**
- **Problem:** Only final transcripts are logged, interim transcripts discarded
- **Impact:** Cannot debug cases where interim transcript triggered action before final transcript corrected it
- **Solution:** Log both interim and final transcripts with confidence scores

**Gap 4: Provider-Specific Error Details**
- **Problem:** Generic "STT failed" error without provider-specific error codes
- **Impact:** Cannot distinguish between rate limits, timeouts, authentication failures, or service outages
- **Solution:** Log provider-specific error codes, retry-after headers, and HTTP status codes

**Gap 5: Network-Level Metrics Missing**
- **Problem:** Application-level metrics don't capture packet loss, jitter, or bandwidth
- **Impact:** Audio quality issues blamed on STT/TTS when root cause is network
- **Solution:** Collect WebRTC stats (packet loss, jitter, RTT) and correlate with call quality

**Gap 6: Cross-Region Latency Not Tracked**
- **Problem:** Metrics show "STT latency: 200ms" but don't separate network vs processing time
- **Impact:** Cannot determine if latency is due to geographic distance or provider performance
- **Solution:** Track network latency separately (ping time to provider) and subtract from total latency

**Gap 7: Cost Attribution Missing**
- **Problem:** Metrics track token counts but don't calculate per-call costs
- **Impact:** Cannot identify expensive calls or optimize cost
- **Solution:** Calculate cost per call: (STT minutes Ã— rate) + (LLM tokens Ã— rate) + (TTS characters Ã— rate)

**Gap 8: Conversation State Not Snapshotted**
- **Problem:** Events show state transitions but don't record full state at each transition
- **Impact:** Cannot replay conversation with correct context, cannot debug state machine issues
- **Solution:** Snapshot conversation state (last N turns, active tools, user context) at each turn boundary

**Gap 9: Barge-In False Positives Not Tracked**
- **Problem:** Barge-in detection triggers but no metric tracks false positives (backchannel, noise)
- **Impact:** Cannot tune barge-in sensitivity, users experience unnecessary interruptions
- **Solution:** Track barge-in events with word count, confidence, and whether it was a valid interruption (requires post-call analysis)

**Gap 10: Provider Failover Not Instrumented**
- **Problem:** Fallback stack activates but no events track which provider failed or why
- **Impact:** Cannot measure failover latency, cannot attribute quality degradation to specific provider
- **Solution:** Emit failover events with: primary provider, failure reason, fallback provider, failover latency

## Common Failure Modes (Observed in Real Systems)

### Correlation ID Propagation Failures

**Symptom:** Events from different components cannot be linked to the same call.

**Root Cause:** Correlation ID not propagated through all service boundaries (HTTP headers, WebSocket messages, message queues).

**Impact:** Debugging requires manual correlation using timestamps and heuristics, which is error-prone and time-consuming.

**Example:** STT service receives audio but doesn't include call-id in request to LLM. LLM traces show orphaned requests that cannot be linked back to specific calls.

**Prevention:** Enforce correlation ID propagation in all service clients. Use OpenTelemetry context propagation automatically.

### Clock Skew Between Services

**Symptom:** Events appear out of order in timeline reconstruction. STT transcript timestamp is after LLM request timestamp.

**Root Cause:** Services use unsynchronized clocks. Clock drift accumulates over time.

**Impact:** Latency calculations are incorrect. Timeline visualization shows impossible causality (effect before cause).

**Example:** STT service clock is 500ms ahead of LLM service clock. Turn latency appears as -200ms (negative latency).

**Prevention:** Use NTP for clock synchronization. Detect clock skew in post-processing and correct timestamps.

### Trace Sampling Drops Critical Calls

**Symptom:** Failed calls are not traced because they were sampled out.

**Root Cause:** Trace sampling (e.g., 10% of calls) is applied uniformly, including to failed calls.

**Impact:** Cannot debug failures because traces are missing.

**Example:** System fails 1% of calls due to provider timeout. With 10% trace sampling, only 0.1% of failures are traced (90% of failures have no trace data).

**Prevention:** Use adaptive sampling: 100% of failed calls, 10% of successful calls. Requires error detection before sampling decision.

### Observability Overhead Causes Latency Spikes

**Symptom:** P99 latency increases when observability is enabled.

**Root Cause:** Excessive logging or tracing overhead (>5% of processing time).

**Impact:** Observability degrades the system it's meant to monitor. Users experience worse latency.

**Example:** Debug-level logs enabled in production, adding 10-20ms per operation. Turn latency increases from P99=800ms to P99=950ms.

**Prevention:** Use appropriate log levels (INFO or WARN in production). Measure observability overhead and keep it <5%.

### Metrics Without Context Are Unactionable

**Symptom:** Alert fires: "P95 turn latency exceeded 800ms" but no information about which component is slow.

**Root Cause:** Metrics are aggregated without component-level breakdown.

**Impact:** Incident response is slow because root cause is unclear. Team wastes time investigating all components.

**Example:** Turn latency alert fires. Team checks STT (normal), LLM (normal), TTS (normal), but doesn't realize network latency to STT provider increased due to regional outage.

**Prevention:** Break down latency metrics by component. Include component tags in metrics: `turn_latency{component="stt"}`, `turn_latency{component="llm"}`, etc.

### Event Storage Costs Spiral Out of Control

**Symptom:** Observability storage costs exceed compute costs.

**Root Cause:** All events stored indefinitely with no retention policy.

**Impact:** Budget overruns. Team forced to reduce observability coverage.

**Example:** 10,000 calls/day Ã— 200 KB/call = 2 GB/day = 60 GB/month = 720 GB/year. At $0.10/GB/month, this is $72/year for first month, $7,200/year for full year (cumulative).

**Prevention:** Implement retention policies: 7 days for all events, 30 days for failed calls, 90 days for sampled calls. Compress old events. Archive to cold storage.

### Missing Interruption State Causes Context Loss

**Symptom:** After barge-in, agent repeats information or loses context.

**Root Cause:** System doesn't track how much audio was delivered before interruption.

**Impact:** Conversation quality degrades. Agent appears forgetful or repetitive.

**Example:** Agent says "Your account balance is five hundred and twenty-three dollars and..." User interrupts at "twenty". System doesn't record that "five hundred and twenty" was delivered. LLM context still includes full sentence, causing confusion.

**Prevention:** Record delivered audio duration and word-level timestamp at interruption point (per D-TT-006). Update LLM context to reflect only delivered audio.

### Provider Errors Not Distinguished

**Symptom:** All provider failures logged as generic "API error".

**Root Cause:** Error handling doesn't preserve provider-specific error codes.

**Impact:** Cannot distinguish between rate limits (retry with backoff), timeouts (retry immediately), and service outages (failover to backup provider).

**Example:** Deepgram returns HTTP 429 (rate limit) but system logs "STT failed" and retries immediately, hitting rate limit again. Should have triggered exponential backoff or failover.

**Prevention:** Log provider-specific error codes, HTTP status codes, and retry-after headers. Implement error-specific handling logic.

## Proven Patterns & Techniques

### Distributed Tracing with OpenTelemetry (Verified)

**Pattern:** Instrument all voice pipeline components with OpenTelemetry SDK.

**Implementation:**
1. Initialize OpenTelemetry tracer at application startup
2. Create root span for each call (call-level span)
3. Create child spans for each turn (turn-level spans)
4. Create child spans for each component operation (STT, LLM, TTS spans)
5. Propagate trace context via W3C TraceContext headers
6. Export spans to observability backend (Jaeger, Grafana, Langfuse)

**Pipecat-Specific:**
- Enable metrics and tracing in `PipelineTask` parameters
- Use turn tracking and conversation ID management
- Add custom span attributes for voice-specific context

**Benefits:**
- Automatic correlation across services
- Standard tooling (Jaeger, Grafana) for visualization
- Low instrumentation overhead (1-5%)

### Adaptive Trace Sampling (Verified)

**Pattern:** Sample 100% of failed calls, 1-10% of successful calls.

**Implementation:**
1. Collect all events in memory during call
2. At call end, determine if call succeeded or failed
3. If failed: export all events
4. If succeeded: export with probability p (e.g., 0.1 for 10%)
5. Include sampling decision in trace metadata

**Rationale:**
- Failed calls are rare (<5%) but critical for debugging
- Successful calls are common (>95%) and less interesting
- Reduces storage costs by 90% while maintaining full failure coverage

**Trade-off:**
- Cannot debug successful calls that were sampled out
- Requires buffering events until sampling decision

### Structured Event Schema (Verified)

**Pattern:** Define strict JSON schema for all events with required fields.

**Required Fields:**
- `event_type`: string (e.g., "stt.transcript.final")
- `timestamp`: ISO 8601 with nanosecond precision
- `call_id`: UUID correlation ID
- `turn_id`: UUID correlation ID (if applicable)
- `trace_id`: OpenTelemetry trace ID
- `span_id`: OpenTelemetry span ID

**Event-Specific Fields:**
- STT events: `text`, `confidence`, `language`, `word_timestamps`
- LLM events: `prompt_tokens`, `completion_tokens`, `model`, `ttft`
- TTS events: `character_count`, `audio_duration`, `voice_id`, `ttfb`

**Benefits:**
- Enables automated parsing and analysis
- Prevents schema drift across services
- Supports schema evolution with versioning

### Latency Breakdown Dashboards (Verified)

**Pattern:** Visualize end-to-end latency as stacked bar chart with component breakdown.

**Metrics:**
- `turn_latency_total`: Total time from user speech end to agent speech start
- `turn_latency_stt`: Time spent in STT processing
- `turn_latency_llm`: Time spent in LLM inference
- `turn_latency_tts`: Time spent in TTS synthesis
- `turn_latency_network`: Time spent in network transit

**Visualization:**
- X-axis: Time (5-minute buckets)
- Y-axis: Latency (milliseconds)
- Stacked bars: Each component as different color
- Overlay: P50, P95, P99 lines

**Benefits:**
- Immediately identifies which component is slow
- Shows latency trends over time
- Enables targeted optimization

### Cost Attribution Per Call (Verified)

**Pattern:** Calculate and log cost for each call based on usage metrics.

**Calculation:**
```
call_cost = (stt_minutes Ã— stt_rate_per_minute) +
            (llm_prompt_tokens Ã— llm_input_rate_per_token) +
            (llm_completion_tokens Ã— llm_output_rate_per_token) +
            (tts_characters Ã— tts_rate_per_character)
```

**Storage:**
- Log cost as metric: `call_cost{call_id="...", bot_id="..."}`
- Aggregate by bot, user, time period
- Alert on anomalous high-cost calls

**Benefits:**
- Identifies expensive calls for optimization
- Enables cost-based capacity planning
- Supports usage-based billing

### Synthetic Monitoring with Canary Calls (Verified)

**Pattern:** Run synthetic test calls every 30-120 seconds from multiple regions.

**Implementation:**
1. Generate synthetic audio (pre-recorded or TTS)
2. Place call to voice agent
3. Measure end-to-end latency and success rate
4. Compare transcript to expected output
5. Alert if latency exceeds threshold or transcript is incorrect

**Metrics:**
- `synthetic_call_success_rate{region="us-east"}`
- `synthetic_call_latency_p95{region="us-east"}`
- `synthetic_call_transcript_accuracy{region="us-east"}`

**Benefits:**
- Detects outages within 1-3 minutes (depending on interval)
- Validates end-to-end functionality
- No dependency on real user traffic

### Word-Level Timestamp Tracking (Verified, Required by D-TS-004)

**Pattern:** Enable word-level timestamps on all TTS requests.

**Data Structure:**
```json
{
  "word": "hello",
  "start_time_ms": 0,
  "end_time_ms": 400,
  "character_start": 0,
  "character_end": 5
}
```

**Usage:**
- At interruption: Find last delivered word using playback position
- Update LLM context: Remove undelivered words from conversation history
- Replay: Synchronize transcript with audio playback

**Benefits:**
- Accurate conversation state after interruptions (per D-TT-006)
- Enables precise replay and debugging
- Required for natural barge-in handling

### Incident Response Runbook Integration (Verified)

**Pattern:** Link observability alerts to incident response runbooks.

**4-Stack Incident Response Framework (Hamming AI, verified):**
1. **Telephony Stack** (50% of incidents): Connection issues, SIP errors, PSTN failures
2. **Audio Stack**: Codec problems, VAD failures, buffer underruns
3. **Intelligence Stack**: LLM failures, tool call errors, prompt issues
4. **Output Stack**: TTS failures, playback errors

**Alert â†’ Runbook Mapping:**
- `stt_error_rate > 5%` â†’ Check Telephony Stack runbook
- `turn_latency_p95 > 800ms` â†’ Check component breakdown, then relevant stack runbook
- `llm_error_rate > 5%` â†’ Check Intelligence Stack runbook
- `tts_error_rate > 5%` â†’ Check Output Stack runbook

**Target Resolution Times:**
- SEV-1: <15 minutes
- SEV-2: <30 minutes
- SEV-3: <2 hours

**Benefits:**
- Faster incident response (don't jump to LLM debugging first)
- Consistent troubleshooting process
- Reduced mean time to resolution (MTTR)

### Post-Call Analytics Pipeline (Verified)

**Pattern:** Async processing of call events for quality analysis.

**Pipeline:**
1. Call ends â†’ Events written to message queue (Kafka, SQS)
2. Consumer processes events:
   - Calculate WER (if ground truth available)
   - Run sentiment analysis on transcript
   - Detect repetitions, interruptions, tool call failures
   - Calculate cost attribution
   - Generate call quality score
3. Write results to data warehouse (BigQuery, Snowflake)
4. Power dashboards and reports

**Benefits:**
- No real-time latency impact
- Enables expensive analysis (sentiment, WER)
- Supports long-term trend analysis

## Engineering Rules (Binding)

### R-OB-001 All Voice Pipeline Components MUST Emit OpenTelemetry Traces
**Rationale:** Distributed tracing is the only way to correlate events across STT, LLM, TTS, and telephony services. Without traces, debugging is guesswork.

**Implementation:**
- Initialize OpenTelemetry SDK in all services
- Create spans for all operations: STT requests, LLM inference, TTS synthesis
- Propagate trace context via W3C TraceContext headers
- Export to observability backend (Jaeger, Grafana, Langfuse, SigNoz)

### R-OB-002 Correlation IDs MUST Be Propagated Through All Service Boundaries
**Rationale:** Without correlation IDs, events from different services cannot be linked to the same call.

**Implementation:**
- Generate call-level correlation ID (UUID) at telephony layer
- Generate turn-level correlation ID at end-of-turn detection
- Include correlation IDs in all HTTP headers, WebSocket messages, message queue metadata
- Store correlation IDs in all events, spans, logs, and metrics

### R-OB-003 TTS Responses MUST Include Word-Level Timestamps
**Rationale:** Required to track delivered audio during interruptions (per D-TS-004 and D-TT-006).

**Implementation:**
- Enable word-level timestamps on every TTS request
- Record word start/end times and character indices
- At interruption: Find last delivered word using playback position
- Update LLM context to reflect only delivered audio

### R-OB-004 Trace Sampling MUST Be Adaptive (100% Failures, 1-10% Successes)
**Rationale:** Failed calls are rare but critical for debugging. Uniform sampling drops most failure traces.

**Implementation:**
- Buffer events in memory during call
- At call end, determine success/failure
- Export all events for failed calls
- Export with probability 0.01-0.1 for successful calls
- Include sampling decision in trace metadata

### R-OB-005 Latency Metrics MUST Include Component-Level Breakdown
**Rationale:** Aggregate latency metrics are unactionable during incidents. Need to know which component is slow.

**Implementation:**
- Emit metrics with component tags: `turn_latency{component="stt"}`, `turn_latency{component="llm"}`, etc.
- Calculate component latency from span durations
- Visualize as stacked bar chart in dashboards
- Alert on component-specific latency thresholds

### R-OB-006 Interruption Events MUST Record Delivered Audio Duration
**Rationale:** Without delivered audio duration, cannot update LLM context accurately after barge-in (per D-TT-006).

**Implementation:**
- At barge-in detection: Record current playback position
- Use word-level timestamps to find last delivered word
- Emit event: `{"event": "tts.interrupted", "delivered_duration_ms": 2300, "total_duration_ms": 5100, "last_delivered_word": "twenty"}`
- Update conversation state to reflect only delivered audio

### R-OB-007 Provider Errors MUST Preserve Error Codes and Retry Metadata
**Rationale:** Generic "API error" prevents distinguishing rate limits, timeouts, and service outages.

**Implementation:**
- Log provider-specific error codes (e.g., Deepgram error codes)
- Log HTTP status codes (429, 503, 504)
- Log retry-after headers
- Emit structured error events: `{"error_type": "rate_limit", "provider": "deepgram", "retry_after_seconds": 5}`

### R-OB-008 Observability Overhead MUST Stay Below 5% of Processing Time
**Rationale:** Excessive observability degrades the system it's meant to monitor.

**Implementation:**
- Measure observability overhead: (time with tracing) - (time without tracing)
- Keep overhead <5% of total processing time
- Use appropriate log levels (INFO or WARN in production, not DEBUG)
- Disable expensive instrumentation in hot paths

### R-OB-009 Event Retention MUST Be Tiered by Call Status
**Rationale:** Storing all events indefinitely causes storage costs to spiral out of control.

**Implementation:**
- 7 days: All events for all calls
- 30 days: All events for failed calls
- 90 days: Sampled events for successful calls (1-10%)
- Archive to cold storage after 90 days
- Compress old events (gzip, zstd)

### R-OB-010 Synthetic Monitoring MUST Run from Multiple Geographic Regions
**Rationale:** Single-region monitoring misses regional outages and geographic latency issues.

**Implementation:**
- Run synthetic test calls every 30-120 seconds from 3+ regions (US, Europe, Asia)
- Measure call success rate and end-to-end latency per region
- Alert on per-region metrics: `synthetic_call_success_rate{region="us-east"} < 0.95`
- Remove failing regions from DNS routing after 3 consecutive failures

## Metrics & Signals to Track

### Core Latency Metrics (Required)

**End-to-End Turn Latency:**
- `turn_latency_p50`: P50 end-to-end turn latency (target: <500ms per D-LT-001)
- `turn_latency_p95`: P95 end-to-end turn latency (target: <800ms per D-LT-001)
- `turn_latency_p99`: P99 end-to-end turn latency (target: <1000ms per D-LT-001)

**Component-Level Latency:**
- `stt_latency_p95`: P95 STT processing latency
- `llm_ttft_p95`: P95 LLM Time to First Token (target: <200ms)
- `tts_ttfb_p95`: P95 TTS Time to First Byte (target: <150ms per D-TS-002)
- `network_latency_p95`: P95 network round-trip time to providers

**Real-Time Factor:**
- `stt_rtf`: STT Real-Time Factor (target: <1.0, alert at >0.8 per D-LT-005)

### Quality Metrics (Required)

**STT Quality:**
- `stt_confidence_avg`: Average word-level confidence score (target: >0.7)
- `stt_low_confidence_rate`: Percentage of transcripts with confidence <0.7
- `stt_wer`: Word Error Rate on test sets (target: 5-10%, alert at >20%)

**LLM Quality:**
- `llm_token_count_avg`: Average completion tokens per response (detect verbosity)
- `llm_repetition_rate`: Percentage of responses with repeated phrases
- `llm_tool_call_success_rate`: Percentage of successful tool/function calls

**Audio Quality:**
- `audio_buffer_underrun_rate`: Percentage of calls with buffer underruns
- `audio_frame_drop_rate`: Percentage of dropped audio frames (target: <1%)
- `audio_jitter_p95`: P95 jitter in milliseconds

### Error Metrics (Required)

**Component Error Rates:**
- `stt_error_rate`: Percentage of STT requests that fail (target: <1%)
- `llm_error_rate`: Percentage of LLM requests that fail (target: <1%)
- `tts_error_rate`: Percentage of TTS requests that fail (target: <1%)

**Provider-Specific Errors:**
- `provider_rate_limit_errors`: Count of HTTP 429 errors per provider
- `provider_timeout_errors`: Count of timeout errors per provider
- `provider_service_errors`: Count of HTTP 503/504 errors per provider

**Interruption Metrics:**
- `barge_in_detection_latency_p95`: P95 barge-in detection latency (target: <100ms)
- `tts_cancellation_latency_p95`: P95 TTS cancellation latency (target: <100ms per D-TS-005)
- `barge_in_false_positive_rate`: Percentage of barge-ins with <2 words (detect false positives)

### Cost Metrics (Required)

**Per-Call Costs:**
- `call_cost_avg`: Average cost per call (STT + LLM + TTS)
- `call_cost_p95`: P95 cost per call (detect expensive outliers)
- `call_cost_total`: Total cost across all calls (for budget tracking)

**Component Costs:**
- `stt_cost_total`: Total STT costs (minutes Ã— rate)
- `llm_cost_total`: Total LLM costs (tokens Ã— rate)
- `tts_cost_total`: Total TTS costs (characters Ã— rate)

### Capacity Metrics (Required)

**Concurrent Load:**
- `active_calls_current`: Current number of active calls
- `active_calls_max`: Maximum concurrent calls in time window
- `provider_concurrent_connections`: Concurrent connections per provider (track against rate limits)

**Throughput:**
- `calls_per_minute`: Call volume per minute
- `turns_per_minute`: Turn volume per minute
- `tokens_per_minute`: LLM token volume per minute

### Synthetic Monitoring Metrics (Required)

**Per-Region Health:**
- `synthetic_call_success_rate{region}`: Success rate of synthetic calls (target: >95%)
- `synthetic_call_latency_p95{region}`: P95 latency of synthetic calls (target: <800ms)
- `synthetic_call_transcript_accuracy{region}`: Transcript match rate (target: >90%)

**Failover Metrics:**
- `provider_failover_count`: Number of failover events per provider
- `provider_failover_latency_p95`: P95 latency of failover transitions (target: <3s)
- `fallback_stack_usage_rate`: Percentage of calls using fallback stack

### User Experience Metrics (Lagging Indicators)

**Call Outcomes:**
- `call_abandonment_rate`: Percentage of calls where user hangs up mid-conversation
- `call_duration_avg`: Average call duration (detect degradation over time)
- `call_retry_rate`: Percentage of users who call back within 5 minutes (detect failures)

**Conversation Quality:**
- `turns_per_call_avg`: Average turns per call (detect conversation length changes)
- `interruptions_per_call_avg`: Average interruptions per call (detect barge-in issues)
- `tool_calls_per_call_avg`: Average tool calls per call (detect agent behavior changes)

## V1 Decisions / Constraints

### In Scope for V1

**Distributed Tracing:**
- OpenTelemetry instrumentation for all voice pipeline components (STT, LLM, TTS)
- Trace context propagation via W3C TraceContext headers
- Call-level and turn-level correlation IDs
- Export to at least one observability backend (Jaeger, Grafana, or SigNoz)

**Core Metrics:**
- End-to-end turn latency (P50, P95, P99)
- Component-level latency breakdown (STT, LLM, TTS)
- Error rates per component and provider
- Cost attribution per call
- Active calls and throughput

**Event Collection:**
- Structured events for all critical operations (STT transcripts, LLM requests, TTS synthesis, interruptions)
- Word-level timestamps from TTS (per D-TS-004)
- Interruption events with delivered audio duration (per D-TT-006)
- Provider-specific error codes and retry metadata

**Synthetic Monitoring:**
- Synthetic test calls every 60-120 seconds from 3 geographic regions
- Per-region success rate and latency tracking
- Alerting on degradation (success rate <95%, P95 latency >800ms)

**Dashboards:**
- Latency breakdown dashboard (stacked bar chart by component)
- Error rate dashboard (per component and provider)
- Cost attribution dashboard (per bot and time period)
- Synthetic monitoring dashboard (per region)

**Adaptive Trace Sampling:**
- 100% of failed calls traced
- 10% of successful calls traced
- Sampling decision at call end

### Out of Scope for V1 (Post-V1)

**Advanced Observability Platforms:**
- Specialized voice AI observability platforms (Hamming AI, Roark)
- AI-powered anomaly detection and root cause analysis
- Automated quality scoring and evaluators
- Multi-speaker analysis (>2 speakers)

**Post-Call Analytics:**
- Async sentiment analysis on transcripts
- WER calculation on production calls (requires ground truth)
- Conversation pattern analysis across calls
- Predictive quality modeling

**Advanced Replay:**
- Full audio recording for all calls (cost prohibitive)
- Video recording for multimodal agents
- Interactive replay with state inspection
- Replay with alternative model versions

**Cost Optimization:**
- Automated cost anomaly detection
- Cost-based routing (route expensive queries to cheaper models)
- Predictive cost modeling based on conversation patterns

**Advanced Alerting:**
- Anomaly detection on latency distributions
- Predictive alerting (alert before SLA breach)
- Automated incident creation and routing
- Integration with PagerDuty, OpsGenie

### Known Limitations

**Trace Sampling Trade-off:**
10% sampling of successful calls means 90% of successful calls have no trace data. Cannot debug sampled-out calls. Requires increasing sampling rate (higher cost) or accepting blind spots.

**Word-Level Timestamp Availability:**
Not all TTS providers support word-level timestamps. OpenAI TTS does not provide word timestamps as of Feb 2026. Cartesia Sonic 3 provides word timestamps. This limits interruption handling accuracy with some providers.

**Observability Storage Costs:**
At 10,000 calls/day with 200 KB events per call, storage costs are ~$72/year for first month, growing to ~$7,200/year cumulative (without retention policies). V1 implements tiered retention (7/30/90 days) to control costs.

**Clock Synchronization:**
Requires NTP for clock synchronization across services. Clock skew >100ms causes incorrect latency calculations. Some cloud environments (serverless) have limited clock synchronization guarantees.

**Real-Time Analytics Limitations:**
Cannot run expensive analysis (sentiment, WER) in real-time without breaking latency budget. Requires async post-call analytics pipeline for quality analysis.

**Provider Error Code Variability:**
Different providers use different error code formats (HTTP status codes, provider-specific codes, error messages). Requires provider-specific error handling logic.

## Open Questions / Risks

### Evidence Gaps

**Pipecat OpenTelemetry Performance:**
Pipecat documentation describes OpenTelemetry support but doesn't provide measured overhead percentages. Claimed 1-5% overhead is based on general OpenTelemetry benchmarks, not Pipecat-specific measurements. V1 must measure actual overhead.

**Optimal Trace Sampling Rate:**
Recommendation of 10% sampling for successful calls is based on general observability practices, not voice-specific data. Optimal rate may be 5% or 20% depending on call volume and debugging needs. Requires experimentation.

**Word-Level Timestamp Accuracy:**
TTS providers claim to provide word-level timestamps but accuracy is not documented. Timestamps may be approximate (Â±50ms) rather than precise. This affects interruption handling accuracy. Requires empirical testing.

**Cost Attribution Accuracy:**
Provider pricing is complex (tiered pricing, volume discounts, regional pricing). Simple cost calculation (usage Ã— rate) may be inaccurate. Requires reconciliation with actual provider invoices.

### Technical Risks

**Correlation ID Propagation Failures:**
If any service in the pipeline doesn't propagate correlation IDs, entire call trace is broken. Requires defensive programming: generate new correlation ID if missing, log warning.

**Trace Context Overhead in WebSocket Messages:**
WebSocket messages are typically small (<1 KB). Adding trace context headers (100-200 bytes) increases message size by 10-20%. May impact bandwidth on high-volume systems. Requires measurement.

**Event Buffering Memory Pressure:**
Adaptive sampling requires buffering events in memory until call end. Long calls (>10 minutes) with high event volume (>1,000 events) may consume significant memory. Requires memory limits and spillover to disk.

**Clock Skew Detection:**
Detecting clock skew requires comparing timestamps from different services. If all services have skewed clocks in the same direction, skew is undetectable. Requires external time source (NTP) for validation.

**Provider Rate Limit on Observability APIs:**
Observability backends (Jaeger, Grafana) have their own rate limits. High event volume may hit backend rate limits, causing dropped traces. Requires backend capacity planning.

### Operational Risks

**Alert Fatigue:**
Too many alerts (low thresholds, noisy metrics) cause alert fatigue. Team ignores alerts, misses real incidents. Requires careful threshold tuning and alert aggregation.

**Dashboard Overload:**
Too many dashboards (one per component, per bot, per region) cause dashboard overload. Team doesn't know which dashboard to check during incidents. Requires consolidated incident response dashboard.

**Observability Blind Spots:**
Gaps in observability (missing correlation IDs, missing word timestamps, missing error codes) cause blind spots. Team cannot debug certain failure modes. Requires continuous observability validation.

**Cost Surprises:**
Observability costs (storage, egress, backend fees) can exceed expectations. High event volume or long retention periods cause budget overruns. Requires cost monitoring and alerts.

**Trace Sampling Bias:**
Sampling decisions based on call success/failure may introduce bias. If failure detection is incorrect (false negatives), failed calls are sampled out. Requires accurate failure detection.

### Hypothesis Requiring Validation

**1-5% Observability Overhead:**
Claimed overhead is based on general OpenTelemetry benchmarks. Actual overhead in voice AI systems with high event volume may be higher (5-10%). Requires measurement in production.

**10% Trace Sampling for Successful Calls:**
Claimed optimal sampling rate is based on general practices. Voice AI systems may require higher sampling (20-30%) due to higher failure rates and debugging needs. Requires experimentation.

**7/30/90 Day Retention Policy:**
Claimed retention periods are based on general practices. Voice AI systems may require longer retention (90/180/365 days) for regulatory compliance or quality analysis. Requires business requirements validation.

**Word-Level Timestamp Precision:**
Assumed that TTS word timestamps are accurate to Â±10ms. Actual precision may be Â±50-100ms depending on provider. This affects interruption handling accuracy. Requires empirical testing.

**Component Latency Breakdown Sufficiency:**
Assumed that STT/LLM/TTS breakdown is sufficient for debugging. May need finer-grained breakdown (network vs processing, prompt vs generation, etc.). Requires incident response experience validation.

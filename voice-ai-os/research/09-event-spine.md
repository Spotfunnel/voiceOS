# Research: Event Spine Architecture for Production Voice AI Systems

**ðŸŸ¢ LOCKED** - Production-validated research based on Hamming.ai five-layer observability (1M+ calls), OpenTelemetry, event sourcing, distributed tracing, 4x faster incident resolution with events vs logs. Updated February 2026.

---

## Why This Matters for V1

Event spine architecture is the observability and debugging foundation that separates functional voice AI prototypes from production-grade systems capable of operating at scale. The core problem: **logs alone cannot debug distributed, real-time voice AI systemsâ€”they lack correlation, ordering guarantees, and replay capability**. Production data from 2025-2026 reveals that teams relying only on logs face three critical failures: (1) **inability to debug**â€”mean time to debug (MTTD) is 2-4 hours because logs from different components (STT, LLM, TTS) cannot be correlated, (2) **inability to analyze**â€”product teams cannot answer questions like "why did task completion drop 15%?" because logs lack structured analytics, and (3) **inability to replay**â€”cannot reproduce production issues locally because logs don't capture causal event sequences.

The industry consensus is unambiguous: **"Voice AI requires events, not just logs"**. Production platforms (LiveKit Agents, OpenTelemetry, Hamming.ai) have all converged on event-driven observability architectures with structured events, correlation IDs, and replay capability. For V1, getting event spine right determines whether production incidents can be debugged in minutes vs hours, whether product teams can analyze conversation quality and task success, and whether engineers can reproduce production issues locally for root cause analysis.

**Critical Production Reality**: Analysis of 1M+ voice AI calls shows that systems with event-driven observability resolve incidents **4x faster** than systems using logs alone. The pattern is clear: **logs tell you what happened, events tell you why it happened and enable you to replay it**.

## What Matters in Production (Facts Only)

### Logs vs Events vs Metrics: Why Logs Alone Fail

**Core Definitions (OpenTelemetry 2026):**

**Logs:**
- Textual, chronological records of events generated by applications
- Provide detailed context about what happened and when
- Essential for diagnosing issues, troubleshooting, security auditing
- **Limitation**: Often unstructured, difficult to query, lack correlation across components

**Events:**
- Discrete, structured occurrences representing specific incidents or changes
- Follow defined schemas with guaranteed information about what happened
- In OpenTelemetry: Events are logs with mandatory name and schema-defined structure
- **Advantage**: Structured, queryable, correlatable, replayable

**Metrics:**
- Numerical data points measuring system state at specific moments (CPU, memory, response times, throughput)
- Compact, efficiently processed, ideal for automated alerting and monitoring
- **Advantage**: Enable quick identification of anomalies and performance issues

**Why Logs Alone Fail for Voice AI:**

1. **No Correlation**: Logs from STT, LLM, TTS are separate streamsâ€”cannot correlate events from single conversation without correlation IDs
2. **No Ordering Guarantees**: Logs may arrive out of order due to buffering, network delaysâ€”cannot reconstruct event sequence reliably
3. **No Replay Capability**: Logs describe what happened but don't capture causal event sequences needed to reproduce issues
4. **Unstructured**: Traditional logs lack schemasâ€”difficult to query for analytics ("what percentage of calls had interruptions?")
5. **No Analytics**: Cannot aggregate logs for product metrics (task success rate, semantic accuracy, user satisfaction)

**Production Evidence (2026):**
- Systems using logs alone: MTTD 2-4 hours, cannot answer product analytics questions
- Systems using events: MTTD 30-60 minutes, full product analytics capability
- **4x faster incident resolution** with event-driven observability (Hamming.ai 2025 analysis)

**The Three Pillars (2026 Standard):**
1. **Metrics**: Quick identification of anomalies (P95 latency spike, error rate increase)
2. **Events**: Structured records enabling correlation, analytics, replay
3. **Logs**: Rich detail for understanding root causes after metrics/events flag issues

**Production Pattern:**
- Metrics for alerting (P95 latency >800ms â†’ alert)
- Events for debugging (trace conversation through event sequence)
- Logs for deep investigation (stack traces, error details)

### Event-Driven Architecture for Voice AI

**Core Insight (2026):**
Voice AI systems are inherently event-driven distributed systems with real-time constraints. Every component (WebRTC, VAD, STT, LLM, TTS) emits events that must be correlated, ordered, and made available for analytics and debugging.

**Event-Driven Architecture Fundamentals:**

**Event Bus/Event Stream:**
- Central bus receives events from multiple sources, routes to destinations through subscriptions
- Google Cloud Eventarc Advanced: Central bus with enrollments (subscriptions) for routing
- Enables centralized visibility and control across distributed services
- Supports real-time event transformation and filtering before delivery

**Event Types in Voice AI (Production Standard 2026):**

**LiveKit Agents Event Types:**
- `user_input_transcribed`: STT completed, transcript available
- `conversation_item_added`: New item added to conversation history
- `function_tools_executed`: Tool call completed
- `metrics_collected`: Performance metrics captured

**Rasa Event Types:**
- `UserUttered`: User spoke, transcript available
- `BotUttered`: Agent spoke, audio delivered
- `ActionExecuted`: Tool/function executed
- Flow-related events: State transitions, intent recognition

**OpenAI Agents SDK Event Types:**
- `VoiceStreamEventAudio`: Streaming audio data
- `VoiceStreamEventLifecycle`: Session/turn management (`turn_started`, `turn_ended`, `session_ended`)
- `VoiceStreamEventError`: Error events with exception information

**Production Event Schema (What to Emit):**

Every event should include:
```json
{
  "event_id": "uuid",
  "event_type": "user_spoke | agent_spoke | tool_executed | state_transition | error",
  "trace_id": "uuid",
  "session_id": "uuid",
  "user_id": "uuid",
  "timestamp": "iso8601",
  "sequence_number": 123,
  "component": "stt | llm | tts | gateway | state_machine",
  "payload": {
    // Event-specific data
  },
  "metadata": {
    "latency_ms": 234,
    "model_version": "gpt-4o-2024-08-06",
    "region": "us-west-2"
  }
}
```

**Critical Fields:**
- **event_id**: Unique identifier for this event
- **trace_id**: Correlation ID for entire conversation (generated at call start)
- **sequence_number**: Monotonically increasing number for ordering
- **timestamp**: When event occurred (ISO8601 format with millisecond precision)

### Event Sourcing and Conversation Replay

**Event Sourcing Pattern:**
Store full series of actions on objects in append-only store rather than just current state. Improves performance, scalability, and auditability.

**Why This Matters for Voice AI:**
- Can reconstruct conversation state at any point in time by replaying events
- Can analyze what led to failures by examining event sequence
- Can test fixes by replaying production event sequences locally
- Can generate analytics by aggregating events (task success rate, interruption rate)

**Production Implementation (2025-2026):**

**Event Store:**
- Append-only database (PostgreSQL with append-only table, or specialized event store)
- Events never modified or deleted (immutable)
- Query by trace_id to get all events for conversation
- Query by event_type to analyze patterns

**Event Replay:**
- Given trace_id, retrieve all events in sequence_number order
- Replay events through system to reconstruct conversation state
- Useful for debugging: Can reproduce production issues locally
- Useful for testing: Can test fixes against real production event sequences

**Optimizations:**
- **Streaming APIs**: Incremental processing for large event sequences
- **Selective filtering**: Filter by event_type to reduce volume
- **Snapshots**: Periodic snapshots of conversation state to avoid replaying millions of events
- **Side effect management**: Avoid duplicate external actions during replay (use dry-run mode)

**Production Example (Quo + Temporal 2025):**
Built real-time AI voice agent (Sona) using Temporal workflow orchestration for managing low-latency, reliable coordination. Temporal provides deterministic execution guarantees and built-in event replay capability.

**Event Replay Use Cases:**
1. **Debugging**: Reproduce production issue locally by replaying events
2. **Testing**: Test fixes against real production event sequences
3. **Analytics**: Aggregate events to generate product metrics
4. **Auditing**: Reconstruct conversation for compliance review
5. **Training**: Use production events to train better models

### Correlation and Ordering Strategies

**Correlation ID (Trace ID) Pattern:**

**Core Concept:**
Generate unique trace ID at start of conversation (audio capture), propagate through all components (STT, LLM, TTS, tools), include in all events, logs, metrics.

**Why This Matters:**
- Enables correlation of events across distributed components
- Can view single conversation as unified trace instead of 5 separate log streams
- Can filter all events for specific conversation using trace_id

**Production Implementation (2026 Standard):**

**W3C TraceContext Specification:**
- Standard format for correlation IDs in HTTP headers
- `traceparent` header: `{version}-{trace-id}-{parent-id}-{trace-flags}`
- `tracestate` header: Vendor-specific context propagation

**OpenTelemetry Context Propagation:**
- Service A includes trace ID and span ID when calling Service B
- Service B creates new span in same trace with A's span as parent
- Enables hierarchical tracing (parent-child relationships between operations)

**Production Pattern:**
```
Call Start (generate trace_id)
  â†’ STT (emit event with trace_id)
    â†’ LLM (emit event with trace_id)
      â†’ Tool Gateway (emit event with trace_id)
        â†’ Tool Execution (emit event with trace_id)
      â†’ LLM Response (emit event with trace_id)
    â†’ TTS (emit event with trace_id)
  â†’ Audio Delivery (emit event with trace_id)
```

**Ordering Strategies:**

**Sequence Numbers:**
- Each event has monotonically increasing sequence number within conversation
- Enables ordering events even if timestamps are slightly off (clock skew)
- Enables detecting missing events (gap in sequence numbers)

**Timestamp Precision:**
- Use ISO8601 format with millisecond precision (or microsecond for high-frequency events)
- Include timezone (UTC recommended)
- **Limitation**: Clock skew between servers can cause ordering issuesâ€”sequence numbers are more reliable

**Causal Ordering:**
- Events have parent-child relationships (e.g., tool_executed is child of llm_requested_tool)
- Use parent_event_id to track causality
- Enables reconstructing causal chain: user_spoke â†’ transcript_ready â†’ llm_invoked â†’ llm_responded â†’ tts_started â†’ agent_spoke

**Production Challenge:**
In distributed systems with concurrent components (STT, LLM, TTS running in parallel), events may arrive out of order. Sequence numbers and causal relationships are critical for correct ordering.

### What Analytics Teams Actually Need

**Production Analytics Requirements (2025-2026):**

**Business Outcome Metrics:**
- **First Call Resolution (FCR)**: Percentage of calls resolved without escalation (target: >85%)
- **Task Success Rate (TSR)**: Percentage of calls achieving user's goal (target: >85%)
- **Containment Rate**: Percentage of calls handled without human agent (target: >70%)
- **Cost Per Interaction**: Total cost per call (target: <$0.20/min for V1)
- **Customer Satisfaction Score (CSAT)**: Post-call user rating (target: >4.0/5.0)

**Conversation Quality Metrics:**
- **Semantic Accuracy Rate**: AI correctly captures meaning beyond word-level transcription (target: 90%+, launch: 80-85%)
- **Intent Recognition Accuracy**: Correct intent identified (target: >90%)
- **Dialog State Maintenance**: Conversation context preserved across turns (target: >95%)
- **Interruption Recovery Rate**: Conversations that successfully recover after interruption (target: >90%)

**Technical Performance Metrics:**
- **Word Error Rate (WER)**: <5% for English enterprise, <10% for accented speech
- **Time to First Audio (TTFA)**: <1.7s (critical if >5s)
- **End-to-End Turn Latency**: P50 <500ms, P95 <800ms (aligns with D-LT-001)
- **Error Rate**: <1% (critical if >5%)
- **Call Success Rate**: >99% (calls that complete without technical failure)

**Operational Metrics:**
- **Average Handle Time (AHT)**: Total call duration (target: 20-30% reduction vs human agents)
- **Agent Utilization Rate**: Percentage of time agent is actively handling calls
- **Call Abandonment Rate**: Percentage of users who hang up before completion (target: <10%)
- **Escalation Rate**: Percentage of calls escalated to human agent (target: <15%)

**ROI and Business Impact:**
- 85% of data, analytics, and IT leaders face C-suite pressure to quantify generative AI ROI (2025)
- Must measure whether agents are actually faster than traditional workflows
- Must measure whether they improve task completion rates and retention
- Winners in 2025-2026 consistently measure, iterate, and optimizeâ€”not just deploy

**What Analytics Teams Cannot Get from Logs Alone:**

1. **Task Success Rate**: Logs show technical success (200 OK) but not semantic success (user's goal achieved)
2. **Semantic Accuracy**: Logs show transcript but not whether meaning was captured correctly
3. **Interruption Recovery**: Logs show interruption occurred but not whether conversation recovered successfully
4. **User Satisfaction**: Logs don't capture user sentiment or post-call ratings
5. **Cost Attribution**: Logs show API calls but not cost per conversation or per customer
6. **Conversation Patterns**: Logs don't enable aggregation ("what percentage of calls have >3 interruptions?")

**Why Events Enable Analytics:**
- **Structured schemas**: Can query and aggregate (e.g., `SELECT AVG(task_success) FROM events WHERE event_type = 'call_ended'`)
- **Correlation**: Can join events across components (e.g., correlate STT confidence with task success)
- **Time-series analysis**: Can track metrics over time (task success rate trending down?)
- **Segmentation**: Can segment by customer, region, agent version (which customers have highest error rate?)

### Distributed Tracing with Correlation IDs

**Core Pattern (2026 Production Standard):**

**Five-Layer Observability Stack (Hamming.ai):**
Voice agents require distributed tracing, not just logging, to correlate events across asynchronous components:

1. **Audio Pipeline**: Frame drops, buffer underruns, jitter, packet loss
2. **STT Processing**: Transcription latency, confidence scores, word error rate, partial vs final timing
3. **LLM Inference**: Token latency, prompt/completion tokens, model version, TTFT
4. **TTS Generation**: Synthesis latency, audio duration, voice ID, TTFB
5. **End-to-End Trace**: Correlation IDs across all layers with total latency breakdown

**Trace ID Generation and Propagation:**
- Generate trace ID (UUID) at audio capture point (call start)
- Propagate through all API calls: STT â†’ LLM â†’ Tool Gateway â†’ Tool â†’ LLM â†’ TTS
- Include trace_id in all events, logs, metrics
- **Performance overhead**: Expect 1-5% latency from tracing instrumentationâ€”worthwhile tradeoff for debugging capability

**OpenTelemetry Context Propagation (2026):**
- Service A includes trace ID and span ID when calling Service B
- Service B creates new span in same trace with A's span as parent
- Enables hierarchical tracing (parent-child relationships between operations)
- **W3C TraceContext Specification**: Standard format for correlation IDs in HTTP headers
  - `traceparent` header: `{version}-{trace-id}-{parent-id}-{trace-flags}`
  - `tracestate` header: Vendor-specific context propagation

**Ordering Strategies:**

**Sequence Numbers (Primary):**
- Each event has monotonically increasing sequence number within conversation
- Enables ordering events even if timestamps are slightly off (clock skew between servers)
- Enables detecting missing events (gap in sequence numbers indicates event loss)

**Timestamp Precision (Secondary):**
- Use ISO8601 format with millisecond precision (or microsecond for high-frequency events)
- Include timezone (UTC recommended)
- **Limitation**: Clock skew between servers can cause ordering issuesâ€”sequence numbers more reliable

**Causal Ordering (Tertiary):**
- Events have parent-child relationships (e.g., `tool_executed` is child of `llm_requested_tool`)
- Use `parent_event_id` to track causality
- Enables reconstructing causal chain: `user_spoke` â†’ `transcript_ready` â†’ `llm_invoked` â†’ `llm_responded` â†’ `tts_started` â†’ `agent_spoke`

**Production Challenge:**
In distributed systems with concurrent components (STT, LLM, TTS running in parallel), events may arrive out of order. Sequence numbers and causal relationships are critical for correct ordering during replay and analytics.

### Event Schema Design

**Production Event Schema (2026 Standard):**

**Required Fields (All Events):**
```json
{
  "event_id": "uuid",
  "event_type": "user_spoke | agent_spoke | tool_executed | state_transition | error",
  "trace_id": "uuid",
  "session_id": "uuid",
  "user_id": "uuid",
  "customer_id": "uuid",
  "timestamp": "2026-02-03T14:23:45.123Z",
  "sequence_number": 123,
  "component": "stt | llm | tts | gateway | state_machine",
  "parent_event_id": "uuid",
  "metadata": {
    "latency_ms": 234,
    "model_version": "gpt-4o-2024-08-06",
    "region": "us-west-2",
    "agent_version": "v1.2.3"
  }
}
```

**Event-Specific Payloads:**

**user_spoke Event:**
```json
{
  "event_type": "user_spoke",
  "payload": {
    "transcript": "I want to book a flight",
    "confidence": 0.95,
    "words": [
      {"word": "I", "confidence": 0.98, "start": 0.0, "end": 0.1},
      {"word": "want", "confidence": 0.96, "start": 0.1, "end": 0.3}
    ],
    "language": "en-US",
    "is_final": true,
    "audio_duration_ms": 1200
  }
}
```

**agent_spoke Event:**
```json
{
  "event_type": "agent_spoke",
  "payload": {
    "text": "I can help you book a flight. Where would you like to go?",
    "audio_duration_ms": 3400,
    "words": [
      {"word": "I", "start": 0.0, "end": 0.1},
      {"word": "can", "start": 0.1, "end": 0.2}
    ],
    "voice_id": "sonic-english-male",
    "was_interrupted": false,
    "delivered_text": "I can help you book a flight. Where would you like to go?"
  }
}
```

**tool_executed Event:**
```json
{
  "event_type": "tool_executed",
  "payload": {
    "tool_name": "search_flights",
    "tool_call_id": "uuid",
    "parameters": {"from": "SFO", "to": "LAX", "date": "2026-03-15"},
    "parameters_hash": "sha256",
    "idempotency_key": "uuid",
    "authorization": {"required": ["booking.read"], "granted": ["booking.read"], "decision": "allow"},
    "validation": {"schema": "pass", "business_rules": "pass"},
    "execution": {
      "started_at": "timestamp",
      "completed_at": "timestamp",
      "latency_ms": 1234,
      "status": "success",
      "result": {"flights": [...]},
      "error": null
    }
  }
}
```

**state_transition Event:**
```json
{
  "event_type": "state_transition",
  "payload": {
    "state_from": "greeting",
    "state_to": "collecting_info",
    "trigger_event": "user_spoke",
    "guards_evaluated": [
      {"guard": "confidence > 0.7", "result": true}
    ],
    "actions_executed": ["log_transition", "update_context"],
    "context": {"user_goal": "book_flight"}
  }
}
```

**error Event:**
```json
{
  "event_type": "error",
  "payload": {
    "error_type": "stt_timeout | llm_error | tts_error | tool_error | state_machine_error",
    "error_message": "STT service timeout after 5s",
    "error_code": "STT_TIMEOUT",
    "component": "stt",
    "stack_trace": "...",
    "recovery_action": "retry_with_fallback",
    "severity": "error | warning | critical"
  }
}
```

### Event Replay for Debugging

**Deterministic Replay Architecture (2026):**

**What is Event Replay?**
Technique for reconstructing and debugging production issues by capturing minimal set of inputs that led to failure, then replaying them locally without live credentials or data exposure.

**Five-Layer Recording:**
1. **Process-level determinism**: Wall-clock time, randomness seeds, thread scheduling, syscalls
2. **Network I/O**: Request/response payloads and timing (pcap, transparent proxy)
3. **Distributed traces**: Interaction sequences across services
4. **Structured events**: Event sequences with causality
5. **Time-travelable execution traces**: Can step through execution, rewind to identify root causes

**Production Benefits:**
- Eliminates "can't reproduce" debugging sessions
- Enables AI debugging agents to work with realistic production scenarios
- Allows team collaboration since exact reproduction is shareable
- Supports session analysis without exposing PII (sanitize before replay)

**Voiceflow Pattern (2025):**
`replay` command recreates previously recorded conversations with consistent inputs for testing, debugging, and demonstrating flows. Recording captures all interactions, then replay processes them sequentially while maintaining user state.

**Time-Travel Debugging:**
Replay provides perfect reproducibility for complex, timing-dependent bugs that normally can't be recreated. Enables developers to add instrumentation retroactively and debug as if issue were failing consistently locally.

**Production Implementation:**

**Event Capture:**
- Store all events in append-only event store (PostgreSQL, specialized event store)
- Include full payload (transcript, LLM response, tool parameters, tool results)
- Sanitize PII before storage (replace with synthetic data)

**Event Replay:**
- Given trace_id, retrieve all events in sequence_number order
- Replay events through system in same order
- Use dry-run mode for tools (simulate execution without side effects)
- Compare replayed state with original state to verify correctness

**Replay Use Cases:**
1. **Bug reproduction**: Reproduce production bug locally by replaying events
2. **Fix validation**: Test fix against real production event sequence, verify issue resolved
3. **Regression testing**: Replay past production conversations to ensure new code doesn't break them
4. **Performance analysis**: Replay with instrumentation to identify latency bottlenecks
5. **Training**: Use production event sequences to train better models or improve prompts

### Analytics Pipeline Architecture

**Production Pattern (Rasa 2025):**

**Analytics Data Pipeline:**
- Events flow from voice AI system to Kafka Event Broker
- Analytics pipeline processes events from Kafka to identify patterns
- Extract markers for solution rate, abandonment rate, custom metrics
- Store aggregated metrics in analytics database (ClickHouse, BigQuery)
- Provide dashboards for product teams (Grafana, Tableau)

**Real-Time vs Batch Analytics:**

**Real-Time (Streaming):**
- Process events as they arrive (sub-second latency)
- Use cases: Live dashboards, real-time alerts, immediate feedback
- Technologies: Kafka Streams, Flink, Spark Streaming
- **Tradeoff**: Higher complexity, higher cost, lower latency

**Batch (Scheduled):**
- Process events in batches (hourly, daily)
- Use cases: Historical analysis, trend identification, reporting
- Technologies: Spark, BigQuery, Snowflake
- **Tradeoff**: Lower complexity, lower cost, higher latency

**Production Recommendation (2026):**
- Use real-time for critical metrics (error rate, P95 latency, call success rate)
- Use batch for historical analysis (weekly task success trends, monthly cost analysis)
- Hybrid approach: Real-time for alerts, batch for deep analytics

**What Product Teams Need (2025-2026 Evidence):**

**From Robylon Analysis (2026):**
1. **Semantic Accuracy Rate**: 80-85% at launch, targeting 90%+ continuously
2. **First Call Resolution (FCR)**: >85% target
3. **Cost Per Interaction**: Track and optimize
4. **Average Handle Time (AHT)**: 20-30% reduction vs human agents
5. **Call Abandonment Rate**: <10% target
6. **Customer Satisfaction Score (CSAT)**: >4.0/5.0 target

**From Hamming.ai Analysis (2026):**
1. **Word Error Rate (WER)**: <10% for English, <15% for Hindi, <12% for German (based on 500K+ interactions)
2. **Time to First Audio**: <1.7s (critical if >5s)
3. **Task Completion Rate**: >85% (critical if <70%)
4. **Error Rate**: <1% (critical if >5%)
5. **Barge-in Recovery**: >90%

**ROI Measurement (Critical for 2026):**
- 85% of data, analytics, and IT leaders face C-suite pressure to quantify generative AI ROI
- Must prove agents are faster than traditional workflows
- Must prove they improve task completion rates and retention
- Cannot rely on deployment-only metrics (uptime, response times)â€”must measure business outcomes

**Analytics Queries Product Teams Run:**

1. **Segmentation**: "What's the task success rate for customer A vs customer B?"
2. **Trend Analysis**: "Is semantic accuracy improving or degrading over time?"
3. **Failure Analysis**: "What percentage of failures are due to STT errors vs LLM errors?"
4. **Cost Analysis**: "Which customers have highest cost per interaction?"
5. **Performance Analysis**: "What's the P95 latency by region?"
6. **Conversation Patterns**: "What percentage of calls have >3 interruptions?"
7. **Feature Usage**: "Which tools are called most frequently?"
8. **User Behavior**: "What percentage of users hang up before task completion?"

**Why Logs Cannot Answer These Questions:**
- Logs are unstructuredâ€”cannot aggregate for percentages, averages, trends
- Logs lack business contextâ€”cannot determine task success from technical logs
- Logs lack correlationâ€”cannot join STT confidence with task success
- Logs lack segmentationâ€”cannot filter by customer, region, agent version

**Events Enable All These Queries:**
- Structured schemas enable aggregation and joins
- Business context included in events (task_success, user_goal, completion_status)
- Correlation via trace_id enables cross-component analysis
- Segmentation via customer_id, region, agent_version

### Event Bus vs Direct Database Writes

**Two Architectural Approaches:**

**Event Bus (Kafka, AWS EventBridge, Google Eventarc):**
- Events published to central bus
- Multiple consumers subscribe to event stream
- Consumers: Analytics pipeline, monitoring system, audit log, data warehouse
- **Benefits**: Decoupling (add new consumers without changing producers), scalability (handle millions of events/sec), replay capability (consumers can replay from any point)
- **Tradeoffs**: Higher complexity, higher operational cost, eventual consistency

**Direct Database Writes (PostgreSQL, ClickHouse):**
- Events written directly to database table
- Consumers query database for events
- **Benefits**: Simpler architecture, lower operational cost, immediate consistency
- **Tradeoffs**: Coupling (consumers must know database schema), lower scalability (database becomes bottleneck), limited replay capability

**Production Recommendation (2026):**

**For V1 (Simple, Reliable):**
- Use direct PostgreSQL writes with append-only events table
- Simpler to operate, lower cost, sufficient for <1000 concurrent calls
- Can migrate to event bus later if needed

**For Scale (>1000 Concurrent Calls):**
- Use event bus (Kafka, AWS EventBridge)
- Required for high throughput, multiple consumers, advanced analytics
- Higher complexity but necessary for scale

**Hybrid Approach:**
- Write events to PostgreSQL for immediate consistency and simple queries
- Publish events to Kafka for real-time analytics and multiple consumers
- Best of both worlds but highest complexity

## Common Failure Modes (Observed in Real Systems)

### 1. Logs Without Correlation IDs
**Symptom**: Production issue occurs but cannot debug. Logs from STT, LLM, TTS are separate streams. Cannot correlate events from single conversation.

**Root cause**: No correlation IDs (trace IDs). Each component logs independently without shared identifier.

**Production impact**: Mean time to debug (MTTD): 2-4 hours. Cannot trace conversation through system. 4x slower incident resolution vs systems with correlation IDs.

**Observed in**: Systems without distributed tracing, systems using basic logging without trace ID propagation.

**Mitigation**:
- Generate trace ID (UUID) at call start
- Propagate through all components via HTTP headers (W3C TraceContext)
- Include trace_id in all events, logs, metrics
- Implement OpenTelemetry for standardized context propagation

---

### 2. Events Without Sequence Numbers
**Symptom**: Events arrive out of order. Cannot reconstruct conversation sequence correctly. Replay produces different results than original.

**Root cause**: No sequence numbers. Relying only on timestamps, which are subject to clock skew.

**Production impact**: 5-10% of replays produce incorrect results due to event ordering issues.

**Observed in**: Systems using timestamps only, systems with distributed components across multiple servers with clock skew.

**Mitigation**:
- Add monotonically increasing sequence number to each event within conversation
- Use sequence numbers as primary ordering mechanism
- Use timestamps as secondary ordering (for human readability)
- Detect missing events by checking for gaps in sequence numbers

---

### 3. Unstructured Events Without Schemas
**Symptom**: Cannot query events for analytics. Cannot aggregate for metrics. Product teams cannot answer questions like "what's the task success rate?"

**Root cause**: Events are unstructured (free-form text) without defined schemas. Cannot parse reliably.

**Production impact**: Product teams cannot measure business outcomes. Cannot optimize based on data. Cannot justify ROI to C-suite.

**Observed in**: Systems using traditional logs, systems without event schema definitions.

**Mitigation**:
- Define event schemas for all event types (user_spoke, agent_spoke, tool_executed, state_transition, error)
- Use JSON Schema or Protobuf for schema enforcement
- Validate events against schema before emission
- Store events in structured format (JSON, Parquet) for analytics

---

### 4. Events Without Causal Relationships
**Symptom**: Cannot understand why event occurred. Cannot trace causal chain from user input to agent response.

**Root cause**: Events don't include parent_event_id. No causal relationships tracked.

**Production impact**: Cannot debug complex issues requiring causal analysis (e.g., "why did LLM call wrong tool?").

**Observed in**: Systems with flat event streams, systems without parent-child relationships.

**Mitigation**:
- Add parent_event_id to each event
- Track causal chain: user_spoke â†’ transcript_ready â†’ llm_invoked â†’ tool_executed â†’ llm_responded â†’ agent_spoke
- Enables reconstructing decision tree for debugging

---

### 5. No Event Replay Capability
**Symptom**: Cannot reproduce production issues locally. Cannot test fixes against real production scenarios.

**Root cause**: Events not stored in append-only store. No replay infrastructure.

**Production impact**: Long debugging time (cannot reproduce issues). Cannot validate fixes against production scenarios.

**Observed in**: Systems without event store, systems using logs only.

**Mitigation**:
- Store all events in append-only event store (PostgreSQL, specialized event store)
- Implement replay capability: Given trace_id, retrieve all events, replay through system
- Use dry-run mode for tools (simulate execution without side effects)
- Sanitize PII before storage for privacy

---

### 6. Events Not Available for Analytics
**Symptom**: Product teams cannot measure business outcomes. Cannot answer questions like "what's the task success rate by customer?"

**Root cause**: Events stored only for debugging, not exposed to analytics pipeline.

**Production impact**: Cannot optimize based on data. Cannot justify ROI to C-suite. Product decisions based on intuition instead of data.

**Observed in**: Systems with events for debugging only, systems without analytics pipeline.

**Mitigation**:
- Publish events to analytics pipeline (Kafka, data warehouse)
- Provide dashboards for product teams (Grafana, Tableau)
- Enable self-service analytics (SQL queries on event data)
- Track business outcome metrics (FCR, TSR, CSAT, cost per interaction)

---

### 7. Event Emission Adds Excessive Latency
**Symptom**: Event emission adds 50-100ms latency per event. Breaks P50 <500ms target (D-LT-001).

**Root cause**: Synchronous event emission. Waiting for database write or Kafka publish before continuing.

**Production impact**: P50 latency increases from 450ms to 600ms. Breaks conversational flow.

**Observed in**: Systems with synchronous event emission, systems without async logging.

**Mitigation**:
- Use async event emission (don't block on database write)
- Buffer events in memory, batch write to database (e.g., every 100ms or 100 events)
- Use fire-and-forget pattern for non-critical events
- Monitor event emission latency, target <5% overhead

---

### 8. Events Not Sanitized for PII
**Symptom**: Events contain PII (names, phone numbers, credit cards). Compliance violations, security incidents.

**Root cause**: No PII sanitization before event emission.

**Production impact**: GDPR violations, HIPAA violations, security audits fail.

**Observed in**: Systems without PII detection, systems storing raw transcripts in events.

**Mitigation**:
- Implement PII detection (regex patterns, NER models)
- Sanitize PII before event emission (replace with tokens like `<NAME>`, `<PHONE>`, `<CARD>`)
- Store mapping of tokens to actual values in secure database (encrypted, access-controlled)
- Provide de-sanitization capability for authorized debugging

---

### 9. Event Store Growing Unbounded
**Symptom**: Event store grows to terabytes. Query performance degrades. Storage costs explode.

**Root cause**: No data retention policy. Events stored indefinitely.

**Production impact**: High storage costs ($1000s/month). Slow queries (>10s for analytics).

**Observed in**: Systems without retention policy, systems without event archival.

**Mitigation**:
- Implement data retention policy: 30 days hot storage (PostgreSQL), 1 year cold storage (S3), then delete
- Archive old events to object storage (S3, GCS) with compression
- Use partitioning (by date) for efficient queries
- Monitor storage size, alert on unexpected growth

---

### 10. Events Not Versioned
**Symptom**: Event schema changes break analytics queries. Cannot parse old events with new code.

**Root cause**: No event versioning. Schema changes applied without backward compatibility.

**Production impact**: Analytics pipeline breaks on schema changes. Historical analysis impossible.

**Observed in**: Systems without event versioning, systems with frequent schema changes.

**Mitigation**:
- Version event schemas (e.g., `user_spoke_v1`, `user_spoke_v2`)
- Include schema_version in each event
- Maintain backward compatibility (new code can parse old events)
- Provide schema migration tools for historical events

## Proven Patterns & Techniques

### 1. Event-Driven Architecture with Central Event Bus
**Pattern**: All components emit events to central event bus. Multiple consumers subscribe to event stream for analytics, monitoring, auditing.

**Implementation**:
- Components emit events to Kafka or PostgreSQL
- Consumers: Analytics pipeline, monitoring system, audit log, data warehouse
- Events include: event_id, event_type, trace_id, session_id, timestamp, sequence_number, payload
- Use async emission to minimize latency impact

**Benefits**:
- **Decoupling**: Add new consumers without changing producers
- **Scalability**: Handle millions of events/sec
- **Replay**: Consumers can replay from any point

**Production examples**:
- Rasa: Kafka Event Broker with Analytics Data Pipeline
- Google Eventarc: Central bus with enrollments for routing
- LiveKit Agents: Built-in event emission with data hooks

**When to use**: All production systems. Required for V1 to enable analytics and debugging.

---

### 2. Distributed Tracing with Five-Layer Observability Stack
**Pattern**: Implement five-layer observability: Audio Pipeline, STT, LLM, TTS, End-to-End Trace. Generate trace ID at call start, propagate through all layers.

**Implementation**:
- Generate trace_id (UUID) at audio capture
- Propagate via HTTP headers (W3C TraceContext)
- Emit events at each layer with trace_id
- Use OpenTelemetry for standardized tracing
- Expect 1-5% latency overhead

**Benefits**:
- **Correlation**: View single conversation as unified trace
- **Debugging**: Trace conversation through all components
- **Performance**: Identify latency bottlenecks per layer

**Production examples**:
- Hamming.ai: Five-layer stack with correlation IDs (1M+ calls analyzed)
- LiveKit Agents: Unified timeline with transcripts, traces, logs, audio
- OpenTelemetry: Industry-standard tracing framework

**When to use**: All production systems. Required for V1 to enable debugging and performance optimization.

---

### 3. Event Sourcing with Append-Only Store
**Pattern**: Store all events in append-only store. Never modify or delete events. Reconstruct conversation state by replaying events.

**Implementation**:
- Use PostgreSQL with append-only table or specialized event store
- Events include: event_id, trace_id, timestamp, sequence_number, event_type, payload
- Query by trace_id to get all events for conversation
- Replay events to reconstruct state at any point in time

**Benefits**:
- **Auditability**: Full history of all events
- **Replay**: Can reproduce production issues
- **Analytics**: Can aggregate events for metrics

**Production examples**:
- Event sourcing pattern (Microsoft Azure)
- Temporal: Deterministic execution with event replay
- Production standard: Append-only event store

**When to use**: All production systems requiring auditability and replay. Required for V1 to enable debugging and compliance.

---

### 4. Sequence Numbers for Event Ordering
**Pattern**: Add monotonically increasing sequence number to each event within conversation. Use sequence numbers as primary ordering mechanism.

**Implementation**:
- Generate sequence number starting at 0 for each conversation
- Increment on each event emission
- Store sequence_number in event
- Order events by sequence_number (primary), timestamp (secondary)
- Detect missing events by checking for gaps in sequence numbers

**Benefits**:
- **Reliable ordering**: Not affected by clock skew
- **Missing event detection**: Gaps indicate event loss
- **Replay correctness**: Events replayed in correct order

**Production examples**:
- Event sourcing systems: Sequence numbers standard practice
- Distributed systems: Lamport timestamps, vector clocks
- Production standard: Sequence numbers for ordering

**When to use**: All production systems with event replay. Required for V1 to ensure correct ordering.

---

### 5. Causal Event Relationships with parent_event_id
**Pattern**: Each event includes parent_event_id to track causal relationships. Enables reconstructing decision tree.

**Implementation**:
- When emitting event, include parent_event_id (ID of event that caused this event)
- Example: `tool_executed` event has `parent_event_id` = `llm_requested_tool` event ID
- Build causal chain: user_spoke â†’ transcript_ready â†’ llm_invoked â†’ tool_executed â†’ llm_responded â†’ agent_spoke
- Query events by parent_event_id to find all child events

**Benefits**:
- **Causal analysis**: Understand why event occurred
- **Debugging**: Trace decision tree from user input to agent response
- **Analytics**: Analyze patterns (e.g., "which user inputs lead to tool errors?")

**Production examples**:
- OpenTelemetry: Span parent-child relationships
- Distributed tracing: Causal relationships standard practice
- Production pattern: parent_event_id in all events

**When to use**: All production systems with complex event flows. Required for V1 to enable causal analysis.

---

### 6. Async Event Emission with Batching
**Pattern**: Emit events asynchronously without blocking. Buffer events in memory, batch write to database every 100ms or 100 events.

**Implementation**:
- Component emits event to in-memory buffer (non-blocking)
- Background thread flushes buffer to database periodically
- Use fire-and-forget for non-critical events
- Ensure critical events (errors, state transitions) are flushed immediately
- Monitor buffer size, alert if buffer grows unbounded

**Benefits**:
- **Low latency**: Event emission doesn't block (target: <5% overhead)
- **High throughput**: Batching reduces database load
- **Reliability**: Can retry failed writes without blocking main flow

**Production examples**:
- Async logging: Standard practice in high-performance systems
- Batching: Reduces database writes by 10-100x
- Production standard: 100ms or 100 events batch window

**When to use**: All production systems with latency constraints. Required for V1 to meet P50 <500ms target.

---

### 7. PII Sanitization Before Event Emission
**Pattern**: Detect and sanitize PII in event payloads before emission. Replace with tokens, store mapping in secure database.

**Implementation**:
- Run PII detection on event payload (regex patterns for phone/email/SSN, NER models for names)
- Replace PII with tokens: `<NAME>`, `<PHONE>`, `<EMAIL>`, `<CARD>`
- Store mapping: `{token: <NAME>, value: "John Doe", trace_id, timestamp}` in encrypted database
- Provide de-sanitization API for authorized debugging (requires authentication)

**Benefits**:
- **Compliance**: GDPR, HIPAA, TCPA compliance
- **Security**: PII not exposed in event store
- **Auditability**: Can reconstruct original conversation for authorized debugging

**Production examples**:
- PII detection: Regex patterns, NER models (spaCy, Presidio)
- Tokenization: Standard practice in compliance-heavy domains
- Production requirement: Sanitize before storage

**When to use**: All production systems handling PII. Required for V1 to ensure compliance.

---

### 8. Event Replay with Dry-Run Mode
**Pattern**: Replay production events locally with dry-run mode for tools (simulate execution without side effects).

**Implementation**:
- Given trace_id, retrieve all events from event store in sequence_number order
- Replay events through system: emit same events in same order
- For tools: Use dry-run mode (return mock results, don't execute actual side effects)
- Compare replayed state with original state to verify correctness
- Log differences for debugging

**Benefits**:
- **Bug reproduction**: Reproduce production bugs locally
- **Fix validation**: Test fixes against real production scenarios
- **Safety**: No side effects (no charges, no emails, no data modification)

**Production examples**:
- Voiceflow: `replay` command for conversation reconstruction
- Temporal: Deterministic execution with replay capability
- Production standard: Dry-run mode for replay

**When to use**: All production systems. Required for V1 to enable bug reproduction and fix validation.

---

### 9. Real-Time Analytics Pipeline with Kafka
**Pattern**: Publish events to Kafka. Analytics pipeline processes events in real-time to generate metrics, dashboards, alerts.

**Implementation**:
- Components publish events to Kafka topics
- Analytics pipeline consumes events (Kafka Streams, Flink, Spark Streaming)
- Extract metrics: task success rate, semantic accuracy, P95 latency, error rate
- Store aggregated metrics in analytics database (ClickHouse, BigQuery)
- Provide dashboards for product teams (Grafana, Tableau)

**Benefits**:
- **Real-time**: Metrics available within seconds of event occurrence
- **Scalability**: Can process millions of events/sec
- **Flexibility**: Can add new metrics without changing event producers

**Production examples**:
- Rasa: Kafka Event Broker with Analytics Data Pipeline
- Production standard: Kafka for real-time analytics
- Alternative: Direct database writes for simpler deployments

**When to use**: Systems with >1000 concurrent calls or requiring real-time analytics. Optional for V1 (can use direct database writes).

---

### 10. Event Versioning with Backward Compatibility
**Pattern**: Version event schemas. Include schema_version in each event. Maintain backward compatibility (new code can parse old events).

**Implementation**:
- Version event schemas: `user_spoke_v1`, `user_spoke_v2`
- Include `schema_version: "v1"` in each event
- When schema changes: Create new version, maintain old version for backward compatibility
- Analytics code handles multiple versions: `if schema_version == "v1": parse_v1() elif schema_version == "v2": parse_v2()`
- Provide schema migration tools for historical events (optional)

**Benefits**:
- **Backward compatibility**: Old events remain parseable
- **Historical analysis**: Can analyze events from months ago
- **Safe evolution**: Can change schemas without breaking analytics

**Production examples**:
- Event versioning: Standard practice in event-sourced systems
- Protobuf: Built-in versioning and backward compatibility
- Production standard: Version all event schemas

**When to use**: All production systems with evolving event schemas. Required for V1 to enable safe schema evolution.

## Engineering Rules (Binding)

### R1: All Components MUST Emit Structured Events
**Rule**: All components (STT, LLM, TTS, Gateway, State Machine) MUST emit structured events with defined schemas. No unstructured logs for critical events.

**Rationale**: Unstructured logs cannot be queried for analytics. Structured events enable correlation, aggregation, replay.

**Implementation**: Define event schemas (JSON Schema). Validate events before emission. Use structured logging (JSON).

**Verification**: All critical events have defined schemas. Can query events for analytics.

---

### R2: All Events MUST Include Trace ID
**Rule**: All events MUST include trace_id (UUID generated at call start). Trace ID propagated through all components via HTTP headers (W3C TraceContext).

**Rationale**: Without trace_id, cannot correlate events from single conversation. MTTD increases 4x.

**Implementation**: Generate trace_id at call start. Propagate via HTTP headers. Include in all events.

**Verification**: All events have trace_id. Can trace single conversation through all components.

---

### R3: All Events MUST Include Sequence Number
**Rule**: All events MUST include monotonically increasing sequence_number within conversation. Use sequence numbers as primary ordering mechanism.

**Rationale**: Timestamps subject to clock skew. Sequence numbers ensure correct ordering for replay.

**Implementation**: Generate sequence_number starting at 0 for each conversation. Increment on each event.

**Verification**: Events ordered by sequence_number produce correct replay results.

---

### R4: All Events MUST Include parent_event_id for Causal Relationships
**Rule**: All events (except initial event) MUST include parent_event_id to track causal relationships.

**Rationale**: Enables causal analysis and debugging (why did this event occur?).

**Implementation**: When emitting event, include parent_event_id (ID of event that caused this event).

**Verification**: Can reconstruct causal chain from user input to agent response.

---

### R5: Events MUST Be Stored in Append-Only Store
**Rule**: All events MUST be stored in append-only event store (PostgreSQL table, specialized event store). Events never modified or deleted.

**Rationale**: Enables replay, auditability, compliance. Immutable event history.

**Implementation**: Use PostgreSQL with append-only table. No UPDATE or DELETE operations on events.

**Verification**: Event store has no UPDATE/DELETE queries. All events immutable.

---

### R6: Event Emission MUST Be Async with <5% Latency Overhead
**Rule**: Event emission MUST be async (non-blocking). Buffer events in memory, batch write to database. Target <5% latency overhead.

**Rationale**: Synchronous emission breaks P50 <500ms target (D-LT-001). Async emission minimizes impact.

**Implementation**: Emit to in-memory buffer. Background thread flushes every 100ms or 100 events.

**Verification**: Measure latency with and without event emission. Verify overhead <5%.

---

### R7: Events MUST Be Sanitized for PII Before Storage
**Rule**: All events MUST be sanitized for PII before storage. Replace PII with tokens, store mapping in encrypted database.

**Rationale**: GDPR, HIPAA, TCPA compliance. Prevents PII exposure in event store.

**Implementation**: Run PII detection (regex, NER). Replace with tokens. Store mapping securely.

**Verification**: Event store contains no PII. Can de-sanitize for authorized debugging.

---

### R8: Event Replay MUST Use Dry-Run Mode for Tools
**Rule**: When replaying events, tools MUST run in dry-run mode (simulate execution without side effects).

**Rationale**: Prevents duplicate charges, duplicate emails, duplicate data modifications during replay.

**Implementation**: Tools check `dry_run` flag. If true, return mock results without executing.

**Verification**: Replay production events. Verify no side effects (no charges, no emails).

---

### R9: Events MUST Be Versioned with schema_version Field
**Rule**: All events MUST include schema_version field. Maintain backward compatibility (new code can parse old events).

**Rationale**: Enables safe schema evolution without breaking historical analytics.

**Implementation**: Include `schema_version: "v1"` in each event. Version schemas on changes.

**Verification**: Analytics code handles multiple schema versions. Can parse events from 6 months ago.

---

### R10: Event Store MUST Have 30-Day Retention Policy
**Rule**: Events stored in hot storage (PostgreSQL) for 30 days, then archived to cold storage (S3) for 1 year, then deleted.

**Rationale**: Balances debugging needs with storage costs. 30 days sufficient for most debugging.

**Implementation**: Automated archival job runs daily. Moves events >30 days old to S3 with compression.

**Verification**: Event store size remains bounded. Can retrieve archived events for historical analysis.

---

### R11: Analytics Pipeline MUST Process Events for Business Metrics
**Rule**: Analytics pipeline MUST process events to generate business metrics: FCR, TSR, CSAT, cost per interaction, semantic accuracy.

**Rationale**: Product teams need business metrics to optimize and justify ROI. Logs alone insufficient.

**Implementation**: Kafka consumer or SQL queries aggregate events. Store metrics in analytics database. Provide dashboards.

**Verification**: Product teams can query business metrics. Dashboards show FCR, TSR, CSAT trends.

---

### R12: Events MUST Enable Conversation Replay for Debugging
**Rule**: Given trace_id, MUST be able to retrieve all events and replay conversation locally.

**Rationale**: Enables bug reproduction and fix validation against real production scenarios.

**Implementation**: Query event store by trace_id, order by sequence_number, replay through system.

**Verification**: Can reproduce production bugs locally via replay.

---

### R13: Event Emission MUST Not Block Critical Path
**Rule**: Event emission MUST NOT block critical path operations (STT, LLM, TTS). Use fire-and-forget for non-critical events.

**Rationale**: Blocking on event emission breaks latency targets.

**Implementation**: Use async emission. Fire-and-forget for non-critical events. Batch writes for efficiency.

**Verification**: Critical path latency not affected by event emission.

---

### R14: Events MUST Include Component and Agent Version
**Rule**: All events MUST include component (stt, llm, tts, gateway) and agent_version (Docker image tag).

**Rationale**: Enables version-specific debugging and performance analysis. Can identify regressions introduced by specific version.

**Implementation**: Include `component` and `agent_version` in event metadata.

**Verification**: Can filter events by component and version. Can analyze performance by version.

---

### R15: Event Bus MUST Support Multiple Consumers
**Rule**: Event bus MUST support multiple consumers subscribing to same event stream (analytics, monitoring, audit, data warehouse).

**Rationale**: Enables decoupling and flexibility. Can add new consumers without changing producers.

**Implementation**: Use Kafka or equivalent with consumer groups. Multiple consumers subscribe to same topic.

**Verification**: Can add new consumer without changing event producers.

## Metrics & Signals to Track

### Event Emission Metrics
- **Event emission rate**: Events per second (typical: 50-200 per conversation)
- **Event emission latency**: Time from event occurrence to emission (target: <10ms)
- **Event emission overhead**: Additional latency from emission (target: <5%)
- **Event buffer size**: Number of events in memory buffer (monitor for growth)
- **Event flush rate**: Number of batch writes to database per second

### Event Store Metrics
- **Event store size**: Total size of event store (monitor for unbounded growth)
- **Event write latency**: P50/P95/P99 time to write event to store (target: P95 <100ms)
- **Event query latency**: P50/P95/P99 time to query events by trace_id (target: P95 <500ms)
- **Event retention**: Number of events in hot storage vs cold storage
- **Event archival rate**: Number of events archived per day

### Correlation Metrics
- **Trace coverage**: Percentage of events with trace_id (target: 100%)
- **Trace ID propagation failures**: Number of events without trace_id (target: 0)
- **Orphaned events**: Events without parent_event_id (except initial events)
- **Correlation success rate**: Percentage of conversations with complete event traces (target: >99%)

### Ordering Metrics
- **Sequence number gaps**: Number of missing events detected via sequence number gaps (target: 0)
- **Out-of-order events**: Events arriving with sequence_number < previous event (indicates ordering issue)
- **Clock skew**: Difference between timestamps and sequence number ordering
- **Event loss rate**: Percentage of events lost in transit (target: <0.1%)

### Replay Metrics
- **Replay success rate**: Percentage of replays producing same result as original (target: >95%)
- **Replay latency**: Time to replay conversation (typical: 10-100x faster than real-time)
- **Replay coverage**: Percentage of conversations that can be replayed (target: 100%)
- **Replay divergence rate**: Percentage of replays producing different results (indicates non-determinism)

### Analytics Metrics
- **Analytics pipeline latency**: Time from event emission to metric availability (real-time: <10s, batch: <1 hour)
- **Analytics query latency**: P50/P95/P99 time for analytics queries (target: P95 <5s)
- **Dashboard refresh rate**: How often dashboards update (real-time: 1-10s, batch: 1 hour)
- **Metric accuracy**: Percentage of metrics matching ground truth (target: >99%)

### PII Sanitization Metrics
- **PII detection rate**: Number of PII instances detected per 1000 events
- **PII sanitization latency**: Time to detect and sanitize PII (target: <50ms)
- **False positive rate**: Percentage of non-PII flagged as PII (target: <5%)
- **False negative rate**: Percentage of PII not detected (target: <1%)
- **De-sanitization requests**: Number of authorized requests to de-sanitize events

### Event Schema Metrics
- **Schema validation failure rate**: Percentage of events failing schema validation (target: <1%)
- **Schema version distribution**: Which schema versions are in use
- **Schema migration rate**: Number of events migrated to new schema version
- **Backward compatibility failures**: Number of times new code cannot parse old events (target: 0)

### Business Outcome Metrics (From Events)
- **First Call Resolution (FCR)**: Percentage of calls resolved without escalation (target: >85%)
- **Task Success Rate (TSR)**: Percentage of calls achieving user's goal (target: >85%)
- **Semantic Accuracy Rate**: AI correctly captures meaning (target: 90%+, launch: 80-85%)
- **Customer Satisfaction Score (CSAT)**: Post-call user rating (target: >4.0/5.0)
- **Cost Per Interaction**: Total cost per call (target: <$0.20/min for V1)
- **Containment Rate**: Percentage of calls handled without human agent (target: >70%)

### Technical Performance Metrics (From Events)
- **Word Error Rate (WER)**: <5% for English enterprise, <10% for accented speech
- **Time to First Audio (TTFA)**: <1.7s (critical if >5s)
- **End-to-End Turn Latency**: P50 <500ms, P95 <800ms (aligns with D-LT-001)
- **Error Rate**: <1% (critical if >5%)
- **Barge-in Recovery Rate**: >90%
- **Interruption Rate**: Percentage of conversations with interruptions (typical: 30-50%)

### Operational Metrics (From Events)
- **Average Handle Time (AHT)**: Total call duration (target: 20-30% reduction vs human agents)
- **Call Abandonment Rate**: Percentage of users who hang up before completion (target: <10%)
- **Escalation Rate**: Percentage of calls escalated to human agent (target: <15%)
- **Agent Utilization Rate**: Percentage of time agent is actively handling calls

## V1 Decisions / Constraints

### D-EV-001 All Components MUST Emit Structured Events to PostgreSQL
**Decision**: All components (STT, LLM, TTS, Gateway, State Machine) emit structured events to PostgreSQL append-only table. Events include: event_id, event_type, trace_id, session_id, timestamp, sequence_number, component, payload.

**Rationale**: Enables correlation, analytics, replay. PostgreSQL simpler than Kafka for V1 (<1000 concurrent calls).

**Constraints**: Must define event schemas for all event types. Must use async emission to minimize latency.

---

### D-EV-002 Trace ID MUST Be Generated at Call Start and Propagated
**Decision**: Generate trace_id (UUID) at call start (audio capture). Propagate through all components via HTTP headers (W3C TraceContext). Include in all events, logs, metrics.

**Rationale**: Enables correlation across components. Aligns with D-AG-006 and Hamming.ai five-layer stack.

**Constraints**: Must implement trace ID propagation in all components. Must use W3C TraceContext standard.

---

### D-EV-003 Events MUST Include Sequence Numbers for Ordering
**Decision**: All events include monotonically increasing sequence_number within conversation. Order events by sequence_number (primary), timestamp (secondary).

**Rationale**: Ensures correct ordering for replay. Not affected by clock skew. Aligns with R3.

**Constraints**: Must generate and track sequence numbers per conversation.

---

### D-EV-004 Events MUST Include parent_event_id for Causal Relationships
**Decision**: All events (except initial event) include parent_event_id to track causal relationships. Enables reconstructing decision tree.

**Rationale**: Enables causal analysis and debugging. Aligns with R4.

**Constraints**: Must track parent event ID when emitting child events.

---

### D-EV-005 Event Emission MUST Be Async with 100ms Batch Window
**Decision**: Emit events to in-memory buffer (non-blocking). Background thread flushes to PostgreSQL every 100ms or 100 events. Target <5% latency overhead.

**Rationale**: Async emission minimizes latency impact. Aligns with D-LT-001 P50 <500ms target and R6.

**Constraints**: Must implement async emission with batching. Must monitor buffer size.

---

### D-EV-006 Events MUST Be Sanitized for PII Before Storage
**Decision**: Run PII detection on event payloads. Replace PII with tokens (`<NAME>`, `<PHONE>`, `<EMAIL>`, `<CARD>`). Store mapping in encrypted database.

**Rationale**: GDPR, HIPAA, TCPA compliance. Aligns with R7.

**Constraints**: Adds ~20-50ms latency for PII detection. Must implement de-sanitization API for authorized debugging.

---

### D-EV-007 Event Replay MUST Support Dry-Run Mode
**Decision**: Implement event replay: Given trace_id, retrieve all events, replay through system. Tools run in dry-run mode (mock results, no side effects).

**Rationale**: Enables bug reproduction and fix validation. Aligns with R8.

**Constraints**: Must implement dry-run mode for all tools. Must store events with sufficient context for replay.

---

### D-EV-008 Events MUST Be Versioned with schema_version Field
**Decision**: All events include `schema_version: "v1"`. Maintain backward compatibility for 2 previous versions.

**Rationale**: Enables safe schema evolution. Aligns with R9.

**Constraints**: Must version schemas on changes. Must handle multiple versions in analytics code.

---

### D-EV-009 Event Store MUST Have 30-Day Hot, 1-Year Cold Retention
**Decision**: Events stored in PostgreSQL for 30 days (hot storage), archived to S3 for 1 year (cold storage), then deleted.

**Rationale**: Balances debugging needs with storage costs. Aligns with R10.

**Constraints**: Must implement automated archival job. Must support querying archived events.

---

### D-EV-010 Analytics Pipeline MUST Generate Business Metrics
**Decision**: Analytics pipeline processes events to generate: FCR, TSR, CSAT, cost per interaction, semantic accuracy, WER, TTFA, error rate. Store in analytics database (PostgreSQL). Provide Grafana dashboards.

**Rationale**: Product teams need business metrics to optimize and justify ROI. Aligns with R11.

**Constraints**: Must implement analytics queries. Must update dashboards in real-time or batch (hourly).

---

### D-EV-011 Event Schemas MUST Be Defined with JSON Schema
**Decision**: Define event schemas using JSON Schema. Validate events against schema before emission. Reject invalid events.

**Rationale**: Ensures event quality. Enables reliable analytics. Aligns with R1.

**Constraints**: Must define schemas for all event types. Must implement schema validation.

---

### D-EV-012 Events MUST Include Component and Agent Version
**Decision**: All events include `component` (stt, llm, tts, gateway, state_machine) and `agent_version` (Docker image tag) in metadata.

**Rationale**: Enables version-specific debugging and performance analysis. Aligns with D-AG-015.

**Constraints**: Must propagate agent_version through all components.

---

### D-EV-013 Event Store MUST Support Query by trace_id and event_type
**Decision**: Event store MUST have indexes on trace_id and event_type for fast queries. Target P95 query latency <500ms.

**Rationale**: Enables fast debugging (query by trace_id) and analytics (query by event_type).

**Constraints**: Must create database indexes. Must monitor query performance.

---

### D-EV-014 Events MUST Be Emitted for All State Transitions
**Decision**: State machine MUST emit event on every state transition: `{state_from, state_to, trigger_event, guards_evaluated, actions_executed}`.

**Rationale**: Enables debugging state machine execution. Aligns with D-SM-008.

**Constraints**: Must emit event from state machine on every transition.

---

### D-EV-015 Event Emission Failures MUST Be Logged and Alerted
**Decision**: If event emission fails (database error, buffer overflow), log failure and alert. Do not block critical path.

**Rationale**: Event emission failures indicate infrastructure issues. Must be detected and resolved quickly.

**Constraints**: Must implement error handling for event emission. Must alert on failures.

## Open Questions / Risks

### Q1: How to Handle Event Store Query Performance at Scale?
**Question**: With millions of events, query performance degrades. How to maintain P95 <500ms query latency?

**Risk**: Slow queries break debugging workflow. Engineers wait minutes for event retrieval.

**Mitigation options**:
- Use database partitioning (by date, by customer_id)
- Create indexes on trace_id, event_type, timestamp, customer_id
- Use specialized event store (EventStoreDB, Apache Kafka)
- Archive old events to reduce hot storage size

**V1 decision**: Use PostgreSQL with partitioning by date. Create indexes on trace_id, event_type. Archive events >30 days old.

---

### Q2: How to Handle Event Schema Evolution?
**Question**: If event schema changes (new field, changed type), how to handle existing events and analytics code?

**Risk**: Schema changes break analytics queries. Cannot parse old events with new code.

**Mitigation options**:
- Version event schemas, maintain backward compatibility
- Use optional fields for new parameters (provide defaults)
- Implement schema migration tools for historical events
- Test schema changes with historical event samples

**V1 decision**: Version schemas. Maintain backward compatibility for 2 previous versions. Use optional fields for new parameters.

---

### Q3: How to Handle Event Emission Failures?
**Question**: If database is unavailable, event emission fails. Should system buffer events in memory or fail?

**Risk**: Buffering in memory risks data loss on crash. Failing breaks observability.

**Mitigation options**:
- Buffer events in memory with size limit (e.g., 10k events)
- If buffer full, drop oldest events (prioritize recent events)
- Alert on buffer growth (indicates database issues)
- Use persistent queue (Redis) for critical events

**V1 decision**: Buffer in memory with 10k event limit. Alert on buffer >5k. Use Redis for critical events (state transitions, errors).

---

### Q4: How to Handle PII in Event Replay?
**Question**: When replaying events for debugging, need original PII to reproduce issue. How to provide access securely?

**Risk**: De-sanitization exposes PII. Compliance violations if not properly controlled.

**Mitigation options**:
- Require authentication and authorization for de-sanitization API
- Log all de-sanitization requests for audit
- Provide de-sanitization only for specific trace_id (not bulk access)
- Implement time-limited access (de-sanitized data expires after 1 hour)

**V1 decision**: Require authentication for de-sanitization. Log all requests. Time-limited access (1 hour). Audit trail for compliance.

---

### Q5: How to Handle Event Replay with External API Calls?
**Question**: When replaying events, tools make external API calls (Stripe, Twilio). Should replay call real APIs or use mocks?

**Risk**: Calling real APIs causes side effects (charges, emails). Using mocks may not reproduce issue.

**Mitigation options**:
- Use dry-run mode for replay (tools return mock results)
- Provide "replay with real APIs" mode for authorized debugging (requires explicit approval)
- Record external API responses during original execution, replay with recorded responses
- Use test API keys for replay (Stripe test mode, Twilio test numbers)

**V1 decision**: Use dry-run mode by default. Provide "replay with real APIs" mode for authorized debugging. Record API responses for accurate replay.

---

### Q6: How to Handle Event Bus Scalability?
**Question**: If event volume grows to millions per second, PostgreSQL becomes bottleneck. When to migrate to Kafka?

**Risk**: Database cannot keep up with event volume. Event emission latency increases, breaking P50 <500ms target.

**Mitigation options**:
- Start with PostgreSQL for V1 (<1000 concurrent calls, ~100k events/hour)
- Monitor event write latency, alert if P95 >100ms
- Migrate to Kafka when event volume exceeds 100k events/hour
- Use hybrid: PostgreSQL for hot queries, Kafka for analytics pipeline

**V1 decision**: Use PostgreSQL for V1. Monitor write latency. Plan Kafka migration for V2 if event volume >100k/hour.

---

### Q7: How to Handle Analytics Query Performance?
**Question**: Analytics queries on millions of events take minutes. How to maintain <5s query latency?

**Risk**: Product teams cannot get timely insights. Dashboards are stale.

**Mitigation options**:
- Use pre-aggregated metrics (materialize views, rollup tables)
- Use analytics database optimized for aggregation (ClickHouse, BigQuery)
- Use caching for common queries (Redis)
- Use sampling for exploratory queries (analyze 10% of events)

**V1 decision**: Use PostgreSQL materialized views for common metrics. Refresh hourly. Use ClickHouse for V2 if query latency >5s.

---

### Q8: How to Handle Event Replay Non-Determinism?
**Question**: Replaying events produces different results than original (e.g., LLM generates different response). How to handle?

**Risk**: Cannot reproduce production bugs. Replay is unreliable.

**Mitigation options**:
- Record LLM responses during original execution, replay with recorded responses (deterministic)
- Use temperature=0 for replay (more deterministic, but not perfect)
- Accept non-determinism for LLM, focus on reproducing state machine and tool execution
- Use replay for testing state machine logic, not LLM outputs

**V1 decision**: Record LLM responses during original execution. Replay with recorded responses for deterministic reproduction.

---

### Q9: How to Handle Event Store Costs?
**Question**: Storing all events costs $100s-$1000s per month. How to reduce costs without losing debugging capability?

**Risk**: High storage costs. CFO pressure to reduce.

**Mitigation options**:
- Implement aggressive retention policy (7 days hot, 30 days cold, then delete)
- Sample events (store 100% of errors, 10% of successes)
- Compress events before archival (gzip, zstd)
- Use cheaper storage (S3 Glacier for long-term archival)

**V1 decision**: 30 days hot, 1 year cold, then delete. Compress before archival. Monitor costs, adjust retention if >$500/month.

---

### Q10: How to Handle Real-Time Analytics Latency?
**Question**: Real-time analytics pipeline has 10-30s latency from event emission to metric availability. Is this acceptable?

**Risk**: Dashboards are stale. Alerts delayed. Cannot respond to incidents quickly.

**Mitigation options**:
- Optimize analytics pipeline (reduce processing time)
- Use streaming analytics (Kafka Streams, Flink) for sub-second latency
- Use pre-aggregated metrics for critical dashboards
- Accept 10-30s latency for non-critical metrics

**V1 decision**: Accept 10-30s latency for V1 (sufficient for most use cases). Use streaming analytics for V2 if sub-second latency required.

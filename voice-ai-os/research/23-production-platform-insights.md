# Research: Production Voice AI Platform Insights (2024-2026)

**ðŸ”´ IN PROGRESS** - Research on real-world production failures, user complaints, and scaling patterns from Vapi, Retell, Voiceflow, LiveKit, and Pipecat examples. Focus on insights NOT covered in existing research files.

---

## Why This Matters for V1

Existing research covers architecture (three-layer separation), telephony infrastructure, and agent patterns. This research fills critical gaps: **what actually breaks in production**, **what users complain about**, and **what patterns work at scale** based on real incident data, user reports, and production deployments from 2024-2026.

**Critical Gap**: Existing research documents "what should work" but lacks "what actually fails" from production systems handling millions of calls. This research provides:
- **Failure modes** observed in production (not theoretical)
- **User experience failures** that make voice AI feel "broken"
- **Scaling challenges** at 1000+ customers
- **Recent evolution** (2024-2026) showing how platforms adapt

---

## Platform: Vapi

**Key Findings:**

1. **Recurring Infrastructure Failures (2024-2025)**
   - **Source**: Vapi status page incidents (Nov 2024 - Jan 2026)
   - **Pattern**: Multiple outages caused by worker unavailability, memory threshold issues, SIP gateway failures
   - **Specific incidents**:
     - March 2025: "vapifault-transport-never-connected" errors due to memory threshold issues, ~2 hour resolution
     - November 2024: WebSocket calls disrupted 43 minutes due to API gateway configuration
     - December 2025: Call workers blocked uploading recordings to downstream object storage outage
     - January 2026: Google Gemini rate limiting causing downtime
   - **Evidence**: [Vapi Status Page](https://status.vapi.ai/incidents)

2. **Concurrent Call Limits**
   - **Source**: Vapi support documentation
   - **Finding**: Default concurrency limit of **10 simultaneous calls** (inbound + outbound combined) at account level
   - **Impact**: Enterprise plans may have different limits, but base limit applies regardless of plan tier
   - **Evidence**: [Vapi Support Forum](https://support.vapi.ai/t/27576617/what-is-the-maximum-number-of-concurrent-calls-in-vapi)

3. **User Complaints: "Not Reliable for Production"**
   - **Source**: Vapi community forum (2024-2025)
   - **Pattern**: Users report lost customers during product demos, dashboard login failures, repeated disruptions
   - **Quote**: "Vapi Suck again" - users describe service as "not reliable for any production" use
   - **Evidence**: [Vapi Community Forum](https://vapi.ai/community/m/1380169001157791845)

**Failure Mode Identified:**

- **What breaks**: Worker unavailability during traffic spikes, downstream service dependencies (object storage, STT providers) causing cascading failures
- **Why it breaks**: 
  - Call workers blocked on I/O operations (uploading recordings) during downstream outages
  - Memory threshold issues causing worker crashes under load
  - No graceful degradation when third-party services fail (Deepgram, ElevenLabs, Cartesia)
- **How to avoid**: 
  - Implement circuit breakers for downstream services (object storage, STT/TTS providers)
  - Set up transcriber fallbacks (Vapi recommends but doesn't enforce)
  - Monitor worker memory usage and implement auto-scaling before thresholds
  - Use async/background processing for non-critical operations (call recording uploads)

**Production Pattern Identified:**

- **Pattern**: Breaking API changes with deprecation warnings (September 2025)
- **Evidence**: Vapi removed deprecated endpoints (`/logs`, `/workflow/{id}`, `/test-suite`, `/knowledge-base`) with breaking changes
- **Applicability**: SpotFunnel should version APIs explicitly, provide migration paths, and deprecate gradually (not remove abruptly)

---

## Platform: Retell AI

**Key Findings:**

1. **Twilio Integration Audio Encoding Mismatch**
   - **Source**: Retell AI Twilio integration tutorial (2024-2025)
   - **Finding**: Twilio requires mulaw 8kHz audio encoding, but Retell's default uses PCM 16kHz
   - **Impact**: Causes garbled audio and dropped packets in production
   - **Root cause**: Developers treat Retell and Twilio as single system when they require separate configuration
   - **Evidence**: [Retell Twilio Integration Tutorial](https://callstack.tech/blog/retell-ai-twilio-integration-tutorial-build-ai-voice-calls-step-by-step)

2. **Latency Troubleshooting Documentation**
   - **Source**: Retell AI reliability documentation
   - **Finding**: Platform provides tools to check estimated vs actual latency, indicating latency is a known pain point
   - **Pattern**: Comprehensive troubleshooting docs suggest these are documented issues, not active outages
   - **Evidence**: [Retell Latency Troubleshooting](https://docs.retellai.com/reliability/troubleshoot-latency)

**Failure Mode Identified:**

- **What breaks**: Audio encoding mismatches between voice AI platform and telephony provider cause garbled audio
- **Why it breaks**: 
  - Platforms assume telephony provider handles encoding automatically
  - Default configurations don't match telephony provider requirements (mulaw 8kHz vs PCM 16kHz)
  - Integration documentation doesn't emphasize encoding requirements upfront
- **How to avoid**: 
  - Validate audio encoding compatibility during onboarding
  - Provide explicit encoding configuration in integration docs
  - Auto-detect telephony provider and set correct encoding defaults
  - Test audio quality in staging before production deployment

**Production Pattern Identified:**

- **Pattern**: Sub-second latency positioning (2025 benchmarks)
- **Evidence**: Retell positions itself competitively on sub-second latency vs Synthflow and Twilio
- **Applicability**: Latency is competitive differentiator - SpotFunnel should measure and optimize end-to-end latency, not just per-component

---

## Platform: Voiceflow

**Key Findings:**

1. **Runtime Performance Improvements (2024)**
   - **Source**: Voiceflow runtime performance improvements documentation
   - **Finding**: JavaScript steps improved 70-94% faster (complex calculations: 868ms â†’ 51ms)
   - **Pattern**: Function steps gained ~50-150ms improvements through code caching
   - **Impact**: API/Function steps reduced occasional 3-second delays to ~0.5s
   - **Evidence**: [Voiceflow Runtime Performance](https://www.voiceflow.com/pathways/runtime-performance-improvements)

2. **Voice Agent Latency Reduction (Late 2024-2025)**
   - **Source**: Voiceflow "Turbocharging Voice Agents" update
   - **Finding**: Real-world voice-to-voice latency reduced by ~700ms (Twilio) and ~1200ms (web voice widgets)
   - **Optimization layers**: ASR transcription improvements, LLM streaming via Agent Step, TTS acceleration
   - **Evidence**: [Voiceflow Turbocharging Update](https://www.voiceflow.com/pathways/turbocharging-voice-agents)

3. **Production Limits**
   - **Source**: Voiceflow troubleshooting documentation
   - **Finding**: 
     - Concurrent calls limited by workspace plan
     - Maximum call duration: 30 minutes
     - Maximum user inactivity: 3 minutes before call ends
   - **Evidence**: [Voiceflow Troubleshooting](https://docs.voiceflow.com/docs/troubleshooting-voice)

**Failure Mode Identified:**

- **What breaks**: Visual builder complexity slows onboarding despite performance improvements
- **Why it breaks**: 
  - Drag-and-drop workflows require understanding of conversation flow logic
  - JavaScript steps historically slow (868ms for complex calculations) - now fixed but indicates complexity
  - Async steps where platform "waits for long-running actions" break conversational flow
- **How to avoid**: 
  - Use declarative objective configuration (not visual workflows) for faster onboarding
  - Avoid async steps that block conversation - use event-driven workflows instead
  - Pre-optimize common operations (caching, code optimization) before users hit performance issues

**Production Pattern Identified:**

- **Pattern**: Streaming API for real-time interactions (October 2024)
- **Evidence**: Voiceflow added real-time event streaming for dynamic conversations, supporting LLM response streaming
- **Applicability**: SpotFunnel should support streaming responses to reduce perceived latency, not wait for full LLM completion

---

## Platform: LiveKit

**Key Findings:**

1. **Stateful Load Balancing for Voice Agents**
   - **Source**: LiveKit Cloud deployment blog (2024-2025)
   - **Finding**: Use stateful load balancing that accounts for effective load (CPU/GPU, memory, user locality), not just connection count
   - **Why**: Voice/video sessions are long-lived and resource-intensive - agents typically support only tens of concurrent sessions per machine
   - **Evidence**: [LiveKit Cloud Deployment](https://blog.livekit.io/deploy-and-scale-agents-on-livekit-cloud)

2. **Graceful Draining During Updates**
   - **Source**: LiveKit agent deployment documentation
   - **Finding**: Allow existing calls to complete while blocking new sessions on outdated versions
   - **Pattern**: Instant rollbacks with single command if issues detected
   - **Evidence**: [LiveKit Agent Deployment](https://docs.livekit.io/agents/v0/deployment)

**Production Pattern Identified:**

- **Pattern**: Stateful load balancing with effective load metrics (CPU/GPU, memory, locality)
- **Evidence**: LiveKit Cloud handles auto-scaling, draining, rollbacks automatically for paid plans
- **Applicability**: SpotFunnel should implement stateful load balancing for voice agents (not stateless HTTP load balancing) - account for resource consumption per session, not just connection count

---

## Platform: Pipecat Examples (Daily.co)

**Key Findings:**

1. **Structured Agent Design with Pipecat Flows**
   - **Source**: Daily.co blog "Beyond the Context Window" (2024-2025)
   - **Finding**: Use Pipecat Flows to create predefined conversation paths instead of relying solely on large context windows
   - **Why**: Prevents "context rot" where instructions get ignored in complex conversations
   - **Pattern**: Keep agents focused on next step instead of overwhelming with all possible instructions at once
   - **Evidence**: [Daily.co Pipecat Flows Blog](https://www.daily.co/blog/beyond-the-context-window-why-your-voice-agent-needs-structure-with-pipecat-flows)

2. **Voicemail Detection Production Example (2025)**
   - **Source**: Daily.co blog "Building a Voicemail Detection Agent"
   - **Finding**: Real-world example demonstrates:
     - Automated outbound calls using Daily's dial-out feature
     - Real-time voicemail detection
     - Dynamic messaging for voicemail vs live conversations
   - **Use cases**: Verification calls for lenders, appointment scheduling
   - **Evidence**: [Daily.co Voicemail Detection](https://www.daily.co/blog/building-a-voicemail-detection-agent-with-pipecat-and-daily/)

**Production Pattern Identified:**

- **Pattern**: Structured conversation flows (Pipecat Flows) prevent context rot
- **Evidence**: Daily.co team recommends flows over pure context-window approaches for production reliability
- **Applicability**: SpotFunnel's three-layer architecture aligns with this - Layer 2 (orchestration) provides structured objective sequencing, preventing LLM from ignoring instructions

---

## Cross-Platform Insights: Real-Time Infrastructure

**Key Findings:**

1. **Latency Budget Breakdown (Production Reality)**
   - **Source**: WebRTC Ventures, Telnyx, Cresta research (2024-2025)
   - **Finding**: Typical voice AI call flow accumulates latency:
     - Network hops alone: 250ms minimum (50ms per service Ã— 5 steps)
     - STT processing: 100-300ms
     - LLM inference: 350-1,000ms
     - TTS synthesis: 90-200ms
     - **Total**: 800ms-1.5+ seconds
   - **Production benchmarks**: Twilio averaging 950ms, Vonage 800-1,200ms
   - **Human threshold**: 200ms natural, 300-500ms feels unnatural, >1.2 seconds causes 40% abandonment
   - **Evidence**: [WebRTC Ventures Latency Guide](https://webrtc.ventures/2025/10/slow-voicebot-how-to-fix-latency-in-voice-enabled-conversational-voice-ai-agents/)

2. **EMEA Region Infrastructure Failures**
   - **Source**: Telnyx research on EMEA voice AI failures (2024-2025)
   - **Finding**: German patients experience 1.2-second delays, Italian calls drop due to cross-border carrier routing
   - **Root cause**: Not AI models, but actual audio path problems through multiple carrier networks
   - **Testing gap**: Controlled environment tests appear fine, but production scale exposes routing, compliance, and latency issues
   - **Evidence**: [Telnyx EMEA Infrastructure](https://telnyx.com/resources/why-voice-ai-fails-in-emea-and-what-infrastructure-has-to-do-with-it)

**Failure Mode Identified:**

- **What breaks**: Regional infrastructure routing causes latency spikes and call failures (EMEA-specific)
- **Why it breaks**: 
  - Cross-border carrier routing adds multiple network hops
  - Testing environments don't replicate production carrier routing
  - Compliance requirements (GDPR, regional data residency) force suboptimal routing
- **How to avoid**: 
  - Deploy regional infrastructure (co-located STT/LLM/TTS with telephony core)
  - Test with production carrier routing, not just controlled environments
  - Monitor latency by region (P50/P95/P99), alert on regional degradation
  - Use co-located GPUs with telephony (Telnyx pattern eliminates 250ms+ network delays)

**Production Pattern Identified:**

- **Pattern**: Co-located infrastructure eliminates network hops
- **Evidence**: Telnyx achieves sub-second response times by colocating GPUs with telephony core
- **Applicability**: SpotFunnel should consider regional deployment with co-located services (STT/LLM/TTS + telephony) to reduce latency 50-100ms per region

---

## Cross-Platform Insights: User Experience Failures

**Key Findings:**

1. **Interruption and Talking Over Users**
   - **Source**: Wired article on ChatGPT voice interruptions, academic research (2024-2025)
   - **Finding**: ChatGPT's voice feature interrupts users mid-thought - described as "like collaborating with an over-caffeinated friend who can't stand even a second of silence"
   - **Structural cause**: Academic research identifies three recurring patterns:
     1. **Temporal Misalignment** - System delays violate conversational rhythm expectations
     2. **Expressive Flattening** - Loss of paralinguistic cues leads to inappropriate literal responses
     3. **Repair Rigidity** - System architecture prevents users from correcting errors in real-time
   - **Research conclusion**: "The unnaturalness of modern speech agents is not a failure of model capacity, but a structural consequence of the pipeline architecture itself"
   - **Evidence**: [Wired ChatGPT Interruptions](https://www.wired.com/story/how-to-stop-chatgpt-talking-over-you/)

2. **Voice Assistant Usability Problems**
   - **Source**: NNGroup research on intelligent assistants (2024)
   - **Finding**: Alexa, Google Assistant, Siri deliver "usability grossly inferior to promised usability"
   - **Pattern**: Work only for simple queries with straightforward answers
   - **Evidence**: [NNGroup Voice Assistant Usability](https://www.nngroup.com/articles/intelligent-assistant-usability/)

**Failure Mode Identified:**

- **What breaks**: Voice AI feels "broken" due to interruptions, talking over users, inability to repair errors
- **Why it breaks**: 
  - Pipeline architecture (STT â†’ LLM â†’ TTS) doesn't coordinate seamlessly
  - Turn-taking logic doesn't account for natural conversation rhythm
  - No real-time error correction mechanism
- **How to avoid**: 
  - Implement Smart Turn detection (Pipecat V3, Silero VAD v6.2) with proper interruption handling
  - Track delivered audio using word-level timestamps (prevent context loss on interruption)
  - Allow users to interrupt and correct errors mid-conversation
  - Test interruption scenarios explicitly (20-30% of conversations have interruptions)

**Production Pattern Identified:**

- **Pattern**: Speculative tool calling to hide latency (2024-2025)
- **Evidence**: Stream.io recommends running filler track (immediate acknowledgment) + speculation track (background API calls) in parallel
- **Applicability**: SpotFunnel should implement speculative execution for workflows - generate conversational filler while executing API calls in background, hide 1.5-2 seconds of tool latency

---

## Cross-Platform Insights: Workflow Integration Anti-Patterns

**Key Findings:**

1. **Conversation Freezes During API Calls**
   - **Source**: Stream.io blog on speculative tool calling (2024-2025)
   - **Finding**: Standard sequential pipelines result in 600-900ms round-trip delays, creating awkward 3-second silences
   - **Anti-pattern**: Blocking on API calls during voice loop freezes conversation entirely
   - **Latency stack**: ASR (300ms) + LLM decision (200-1000ms) + Tool execution (100-2000ms) + LLM response (300ms) + TTS (250-300ms)
   - **Evidence**: [Stream.io Speculative Tool Calling](https://getstream.io/blog/speculative-tool-calling-voice/)

2. **Production Architecture Requirements**
   - **Source**: Telnyx "How to Build Voice AI That Doesn't Fall Apart" (2024-2025)
   - **Finding**: Voice AI systems fail in production due to architectural issues:
     - Feel slow in live conversations
     - Break when callers interrupt
     - Lose context across turns
     - Cannot safely execute real actions
   - **Root cause**: Teams wire together LLM and telephony provider without addressing systemic challenges
   - **Evidence**: [Telnyx Voice AI Production Guide](https://telnyx.com/resources/how-to-build-a-voice-ai-product-that-does-not-fall-apart-on-real-calls)

**Failure Mode Identified:**

- **What breaks**: Conversations freeze during workflow API calls (3-second silences)
- **Why it breaks**: 
  - Sequential pipeline waits for tool execution before continuing
  - No speculative execution or parallel processing
  - Workflow calls block conversation thread
- **How to avoid**: 
  - Use event-driven workflows (async, fire-and-forget)
  - Implement speculative tool calling (filler track + speculation track)
  - Never block conversation on workflow execution
  - Use progressive disclosure ("Let me check that...") while executing in background

**Production Pattern Identified:**

- **Pattern**: Event-driven routing for real-time voice (lowest latency)
- **Evidence**: Telnyx research identifies event-driven routing as non-negotiable for voice (latency above 300ms breaks flow)
- **Applicability**: SpotFunnel's three-layer architecture already separates workflows (Layer 3) from conversation (Layer 1+2) - this validates the design

---

## Cross-Platform Insights: Scaling Challenges

**Key Findings:**

1. **Orchestration as Primary Challenge**
   - **Source**: Telnyx AI orchestration research (2024-2025)
   - **Finding**: 94% of organizations recognize process orchestration as essential, but 69% have projects that failed to reach operational deployment
   - **Pattern**: Only 11% achieved full AI deployment despite 65% moving to pilot programs
   - **Root cause**: Technology integration challenges, not model quality
   - **Evidence**: [Telnyx AI Orchestration](https://telnyx.com/resources/ai-orchestration-platforms-best-practices)

2. **Multi-Tenancy Architecture Requirements**
   - **Source**: AWS multi-tenant generative AI guidance (2024-2025)
   - **Finding**: Multi-tenant platforms must address:
     - Tenant isolation and identity management
     - Data ownership (tenant separation in shared infrastructure)
     - Noisy neighbor prevention
     - Training and testing validation
   - **Evidence**: [AWS Multi-Tenant AI Guidance](https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-multitenant/agentic-ai-multitenant.pdf)

**Failure Mode Identified:**

- **What breaks**: Scaling to 1000+ customers fails due to orchestration complexity, not model quality
- **Why it breaks**: 
  - Teams don't separate orchestration from conversation logic
  - Multi-tenancy not designed from V1 (added later causes isolation issues)
  - Noisy neighbor problems (one customer's load affects others)
- **How to avoid**: 
  - Design three-layer architecture from V1 (voice core, orchestration, workflows)
  - Implement tenant isolation with runtime context injection
  - Use base agent + customer overlay pattern (not per-customer code)
  - Monitor per-tenant resource consumption, implement rate limiting

**Production Pattern Identified:**

- **Pattern**: Base agent + customer overlay for multi-tenancy
- **Evidence**: AWS 2026 guidance recommends immutable base agent with tenant-specific overlays
- **Applicability**: SpotFunnel's architecture already follows this pattern - validates design decision

---

## Cross-Platform Insights: Incident Response Framework

**Key Findings:**

1. **4-Stack Incident Response Framework**
   - **Source**: Hamming.ai voice agent incident response runbook (2024-2025)
   - **Finding**: Structured framework prioritizes troubleshooting by layer:
     1. **Telephony** (SIP registration, network) - target <5 min resolution
     2. **Audio** (codec, WebRTC, VAD) - target <10 min
     3. **Intelligence** (LLM endpoint, prompts) - target <15 min
     4. **Output** (TTS service, audio encoding) - target <10 min
   - **Critical insight**: **50% of voice agent incidents occur in first two stacks (Telephony/Audio)**, not LLM layer
   - **Evidence base**: Analysis of 1M+ production calls across 50+ deployments (Retell, VAPI, Bland, LiveKit)
   - **Evidence**: [Hamming.ai Incident Response](https://hamming.ai/resources/voice-agent-incident-response-runbook)

2. **End-to-End Observability Requirements**
   - **Source**: Hamming.ai observability guide (2024-2025)
   - **Finding**: Voice agents require distributed tracing across all asynchronous components
   - **Pattern**: Propagate trace IDs from audio capture through STT, LLM, TTS layers
   - **Overhead**: Only 1-5% latency overhead for tracing infrastructure
   - **Evidence**: [Hamming.ai Observability](https://hamming.ai/resources/voice-agent-observability-tracing-guide)

**Failure Mode Identified:**

- **What breaks**: Teams jump to LLM debugging first, wasting 2-3x more time resolving incidents
- **Why it breaks**: 
  - Assumption that "AI not working" = LLM problem
  - No structured incident response framework
  - Telephony failures present as "AI not working" to users
- **How to avoid**: 
  - Use 4-Stack Incident Response Framework (start at Stack 1: Telephony)
  - Monitor telephony layer separately from AI layer
  - Implement distributed tracing with correlation IDs
  - Target resolution times: Telephony <5 min, Audio <10 min, Intelligence <15 min, Output <10 min

**Production Pattern Identified:**

- **Pattern**: Distributed tracing with correlation IDs (1-5% overhead)
- **Evidence**: Hamming.ai framework based on 1M+ calls analyzed, teams using structured response resolve issues 4x faster
- **Applicability**: SpotFunnel should implement distributed tracing from V1 - trace IDs from audio capture through all layers, enable end-to-end debugging

---

## Recent Updates (2024-2026) Showing Evolution

**Vapi (2025-2026):**
- Breaking API changes (September 2025): Removed deprecated endpoints with breaking changes
- Enhanced transcription with smart endpointing
- New evaluation system for agent testing
- Voicemail detection improvements
- SMS & Chat integration added
- New CLI tool for developers

**Voiceflow (2024-2025):**
- Runtime performance improvements: JavaScript steps 70-94% faster
- Streaming API for real-time interactions (October 2024)
- Analytics API for programmatic access (August 2025)
- Voice agent latency reduced ~700ms (Twilio) and ~1200ms (web)

**Retell AI (2024-2025):**
- Sub-second latency positioning in benchmarks
- Comprehensive troubleshooting documentation
- Twilio integration improvements (encoding fixes)

**Pipecat/Daily.co (2024-2025):**
- Structured agent design with Pipecat Flows
- Production-ready voicemail detection examples
- WebRTC integration improvements

**Industry-Wide Evolution:**
- Shift toward structured conversation flows (not pure context-window approaches)
- Focus on latency optimization (sub-second targets)
- Emphasis on production reliability (incident response frameworks)
- Multi-tenancy architecture patterns (base agent + overlay)

---

## Synthesis: New Insights NOT Covered in Existing Research

### Insight 1: Worker Unavailability During Traffic Spikes

**What Existing Research Misses:**
- Existing research covers telephony infrastructure but doesn't document worker-level failures
- Vapi incidents show workers crash due to memory thresholds, get blocked on I/O operations

**New Finding:**
- Call workers blocked uploading recordings to object storage during downstream outages
- Memory threshold issues cause worker crashes under load
- No graceful degradation when third-party services fail

**Actionable Pattern:**
- Implement circuit breakers for downstream services
- Use async/background processing for non-critical operations
- Monitor worker memory and auto-scale before thresholds
- Set up provider fallbacks (STT/TTS) - don't just recommend, enforce

---

### Insight 2: Audio Encoding Mismatches Between Platforms

**What Existing Research Misses:**
- Existing research covers codec selection (Opus vs G.711) but doesn't document encoding mismatches
- Retell-Twilio integration shows mulaw 8kHz vs PCM 16kHz mismatch causes garbled audio

**New Finding:**
- Platforms assume telephony provider handles encoding automatically
- Default configurations don't match provider requirements
- Integration docs don't emphasize encoding upfront

**Actionable Pattern:**
- Validate audio encoding compatibility during onboarding
- Auto-detect telephony provider and set correct encoding defaults
- Test audio quality in staging before production
- Document encoding requirements prominently

---

### Insight 3: Visual Builder Complexity Slows Onboarding

**What Existing Research Misses:**
- Existing research covers three-layer architecture benefits but doesn't document visual builder failures
- Voiceflow shows JavaScript steps historically slow (868ms) - now fixed but indicates complexity

**New Finding:**
- Drag-and-drop workflows require understanding conversation flow logic
- Async steps that "wait for long-running actions" break conversational flow
- Performance improvements (70-94% faster) came after users hit issues

**Actionable Pattern:**
- Use declarative objective configuration (not visual workflows) for faster onboarding
- Avoid async steps that block conversation - use event-driven workflows
- Pre-optimize common operations before users hit performance issues

---

### Insight 4: Speculative Tool Calling to Hide Latency

**What Existing Research Misses:**
- Existing research covers workflow async patterns but doesn't document speculative execution
- Stream.io recommends filler track + speculation track pattern

**New Finding:**
- Standard sequential pipelines create 3-second silences
- Blocking on API calls freezes conversation entirely
- Speculative execution hides 1.5-2 seconds of tool latency

**Actionable Pattern:**
- Implement speculative tool calling: filler track (immediate acknowledgment) + speculation track (background API calls)
- Never block conversation on workflow execution
- Use progressive disclosure ("Let me check that...") while executing in background

---

### Insight 5: 4-Stack Incident Response Framework

**What Existing Research Misses:**
- Existing research covers monitoring but doesn't document structured incident response
- Hamming.ai framework shows 50% of incidents in Telephony/Audio, not LLM

**New Finding:**
- Teams jump to LLM debugging first, wasting 2-3x more time
- Telephony failures present as "AI not working" to users
- Structured response resolves issues 4x faster

**Actionable Pattern:**
- Use 4-Stack framework: Telephony â†’ Audio â†’ Intelligence â†’ Output
- Monitor telephony layer separately from AI layer
- Target resolution times: <5 min (Telephony), <10 min (Audio), <15 min (Intelligence), <10 min (Output)

---

## Engineering Rules (Binding)

### R-PROD-001: Implement Circuit Breakers for Downstream Services
**Rule**: All downstream service calls (STT, TTS, object storage, workflows) MUST use circuit breakers with automatic fallback.

**Rationale**: Vapi incidents show workers blocked on I/O operations during downstream outages. Circuit breakers prevent cascading failures.

**Implementation**: 
- Circuit breaker opens after 3 consecutive failures
- Automatic fallback to secondary provider (STT/TTS)
- Background retry with exponential backoff
- Never block conversation on downstream failures

**Verification**: Test with simulated downstream outages. Verify conversation continues with fallback providers.

---

### R-PROD-002: Validate Audio Encoding Compatibility During Onboarding
**Rule**: Audio encoding MUST be validated against telephony provider requirements during customer onboarding. Auto-detect provider and set correct defaults.

**Rationale**: Retell-Twilio integration shows encoding mismatches (mulaw 8kHz vs PCM 16kHz) cause garbled audio in production.

**Implementation**:
- Detect telephony provider during onboarding
- Set encoding defaults based on provider (Twilio: mulaw 8kHz, WebRTC: Opus)
- Test audio quality in staging before production
- Document encoding requirements prominently

**Verification**: Audio quality tests pass in staging before production deployment.

---

### R-PROD-003: Implement Speculative Tool Calling for Workflows
**Rule**: Workflow API calls MUST execute speculatively (background) while conversation continues with filler acknowledgment.

**Rationale**: Sequential pipelines create 3-second silences. Speculative execution hides 1.5-2 seconds of tool latency.

**Implementation**:
- Generate immediate conversational acknowledgment ("Let me check that...")
- Execute workflow API calls in background (speculation track)
- Stream acknowledgment to TTS while API executes
- Never block conversation on workflow execution

**Verification**: Test with slow API calls (2+ seconds). Verify conversation continues without freezing.

---

### R-PROD-004: Use 4-Stack Incident Response Framework
**Rule**: All production incidents MUST be diagnosed using 4-Stack framework: Telephony â†’ Audio â†’ Intelligence â†’ Output. Start at Stack 1, not Stack 3.

**Rationale**: 50% of incidents occur in Telephony/Audio layers, not LLM. Teams jumping to LLM debugging waste 2-3x more time.

**Implementation**:
- Monitor telephony layer separately from AI layer
- Target resolution times: Telephony <5 min, Audio <10 min, Intelligence <15 min, Output <10 min
- Implement distributed tracing with correlation IDs
- Document incident response runbook with 4-Stack framework

**Verification**: Incident resolution times meet targets. Telephony incidents resolved <5 min.

---

### R-PROD-005: Monitor Worker Memory and Auto-Scale Before Thresholds
**Rule**: Call worker memory usage MUST be monitored with auto-scaling triggered before memory thresholds.

**Rationale**: Vapi incidents show workers crash due to memory threshold issues under load. Prevent crashes with proactive scaling.

**Implementation**:
- Monitor worker memory usage (P50/P95/P99)
- Auto-scale when memory exceeds 70% threshold
- Alert on memory >80% threshold
- Test worker behavior under load (chaos engineering)

**Verification**: Load tests show auto-scaling triggers before memory thresholds. No worker crashes under load.

---

## Metrics & Signals to Track

### Worker Health Metrics
- **Worker memory usage**: P50/P95/P99 percentage (target: <70%, alert: >80%)
- **Worker availability**: Percentage of workers available vs total (target: >99%)
- **Worker crash rate**: Crashes per 1000 calls (target: <0.1%)
- **Downstream service failures**: Circuit breaker opens per 1000 calls (target: <1%)

### Audio Encoding Metrics
- **Encoding compatibility**: Percentage of calls with correct encoding (target: 100%)
- **Audio quality degradation**: MOS score degradation due to encoding (target: <0.1 MOS)
- **Encoding mismatch incidents**: Calls with garbled audio due to encoding (target: 0)

### Workflow Latency Metrics
- **Workflow execution time**: P50/P95/P99 latency for workflow API calls
- **Conversation freeze incidents**: Calls with >2 second silence during workflows (target: <1%)
- **Speculative execution success rate**: Percentage of workflows executed speculatively (target: >95%)

### Incident Response Metrics
- **Time to diagnosis**: Time from incident report to root cause identification (target: <5 min Telephony, <10 min Audio, <15 min Intelligence)
- **Time to resolution**: Time from diagnosis to fix (target: <15 min SEV-1, <30 min SEV-2)
- **Incident by stack**: Percentage of incidents in Telephony vs Audio vs Intelligence vs Output (expected: 50% Telephony/Audio)

---

## V1 Decisions / Constraints

### D-PROD-001: Implement Circuit Breakers for All Downstream Services
**Decision**: V1 MUST implement circuit breakers for STT, TTS, object storage, and workflow services with automatic fallback.

**Rationale**: Vapi incidents show workers blocked on I/O operations. Circuit breakers prevent cascading failures.

**Constraints**: Requires fallback provider configuration for STT/TTS. Adds complexity but prevents outages.

---

### D-PROD-002: Auto-Detect Telephony Provider and Set Encoding Defaults
**Decision**: V1 MUST auto-detect telephony provider during onboarding and set correct audio encoding defaults.

**Rationale**: Retell-Twilio integration shows encoding mismatches cause garbled audio. Prevention is better than debugging.

**Constraints**: Requires provider detection logic. Must support Twilio (mulaw 8kHz), WebRTC (Opus), SIP (G.711).

---

### D-PROD-003: Implement Speculative Tool Calling for Workflows
**Decision**: V1 MUST execute workflow API calls speculatively (background) while conversation continues with filler acknowledgment.

**Rationale**: Sequential pipelines create 3-second silences. Speculative execution hides latency.

**Constraints**: Requires parallel execution infrastructure. Adds complexity but improves UX.

---

### D-PROD-004: Use 4-Stack Incident Response Framework
**Decision**: V1 MUST use 4-Stack framework for incident response: Telephony â†’ Audio â†’ Intelligence â†’ Output.

**Rationale**: 50% of incidents in Telephony/Audio, not LLM. Structured response resolves 4x faster.

**Constraints**: Requires monitoring infrastructure for each stack. Must train team on framework.

---

### D-PROD-005: Monitor Worker Memory with Auto-Scaling
**Decision**: V1 MUST monitor worker memory usage and auto-scale before 70% threshold.

**Rationale**: Vapi incidents show workers crash due to memory thresholds. Proactive scaling prevents crashes.

**Constraints**: Requires auto-scaling infrastructure. Must test under load.

---

## Open Questions / Risks

### Q-PROD-001: How to Handle Circuit Breaker Fallback Provider Failures?
**Question**: If primary and fallback STT providers both fail, should conversation continue with degraded quality or fail gracefully?

**Risk**: All providers fail simultaneously, conversation cannot continue.

**Mitigation**: Implement tertiary fallback provider. If all fail, fail gracefully with user notification.

**V1 decision**: Implement tertiary fallback. If all fail, fail gracefully with clear error message.

---

### Q-PROD-002: How to Test Speculative Tool Calling?
**Question**: How to verify speculative execution works correctly without blocking conversation?

**Risk**: Speculative execution bugs cause incorrect workflow execution or conversation inconsistencies.

**Mitigation**: Test with slow API calls (2+ seconds). Verify conversation continues without freezing. Monitor workflow success rate.

**V1 decision**: Test speculative execution in staging with slow API calls. Monitor workflow success rate in production.

---

### Q-PROD-003: How to Train Team on 4-Stack Incident Response?
**Question**: How to ensure team follows 4-Stack framework instead of jumping to LLM debugging?

**Risk**: Team ignores framework, wastes time debugging wrong layer.

**Mitigation**: Create incident response runbook with 4-Stack framework. Require diagnosis at each stack before moving to next. Track incident resolution times.

**V1 decision**: Create runbook, require diagnosis at each stack, track resolution times.

---

## References

1. Vapi Status Page Incidents (Nov 2024 - Jan 2026): https://status.vapi.ai/incidents
2. Vapi Community Forum Complaints: https://vapi.ai/community/m/1380169001157791845
3. Retell AI Twilio Integration Tutorial: https://callstack.tech/blog/retell-ai-twilio-integration-tutorial-build-ai-voice-calls-step-by-step
4. Voiceflow Runtime Performance Improvements: https://www.voiceflow.com/pathways/runtime-performance-improvements
5. LiveKit Cloud Deployment: https://blog.livekit.io/deploy-and-scale-agents-on-livekit-cloud
6. Daily.co Pipecat Flows Blog: https://www.daily.co/blog/beyond-the-context-window-why-your-voice-agent-needs-structure-with-pipecat-flows
7. WebRTC Ventures Latency Guide: https://webrtc.ventures/2025/10/slow-voicebot-how-to-fix-latency-in-voice-enabled-conversational-voice-ai-agents/
8. Telnyx EMEA Infrastructure: https://telnyx.com/resources/why-voice-ai-fails-in-emea-and-what-infrastructure-has-to-do-with-it
9. Wired ChatGPT Interruptions: https://www.wired.com/story/how-to-stop-chatgpt-talking-over-you/
10. Stream.io Speculative Tool Calling: https://getstream.io/blog/speculative-tool-calling-voice/
11. Telnyx Voice AI Production Guide: https://telnyx.com/resources/how-to-build-a-voice-ai-product-that-does-not-fall-apart-on-real-calls
12. Hamming.ai Incident Response: https://hamming.ai/resources/voice-agent-incident-response-runbook
13. Hamming.ai Observability: https://hamming.ai/resources/voice-agent-observability-tracing-guide
14. AWS Multi-Tenant AI Guidance: https://docs.aws.amazon.com/pdfs/prescriptive-guidance/latest/agentic-ai-multitenant/agentic-ai-multitenant.pdf
